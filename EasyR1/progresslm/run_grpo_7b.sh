#!/bin/bash

set -x
# set -euxo pipefail

# ===== ğŸŸ¢ è·¯å¾„è®¾ç½® =====
# 7B æ¨¡å‹è·¯å¾„éœ€è¦æ¯” 3B æ›´å°çš„ batch é…ç½®
MODEL_PATH="/projects/p32958/Results/full_model/qwen25vl_7b_sft"

# è‡ªåŠ¨ç”Ÿæˆæ—¶é—´æˆ³
TIMESTAMP=$(date +"%Y%m%d-%H%M%S")

# ===== ğŸŸ¢ wandb è®¾ç½® =====
# export WANDB_API_KEY=""
export WANDB_API_KEY=""
export WANDB_PROJECT="progresslm_grpo_7b"
export WANDB_RUN_GROUP="qwen2_5_vl_7b_progresslm_grpo"
export WANDB_NAME="visual_demo_qwen2p5vl7b_${TIMESTAMP}"
export WANDB_MODE="online"
export WANDB_DIR="/projects/p32958/Results/wandb_logs"

# ===== ğŸ”´ ç»Ÿä¸€ç¼“å­˜ç›®å½•è®¾ç½®ï¼ˆé¿å…ç£ç›˜é…é¢è¶…é™ï¼‰ =====
CACHE_ROOT="/gpfs/projects/p32876/chengxuan/.cache"

# HuggingFace ç¼“å­˜
export HF_HOME="$CACHE_ROOT/huggingface"
export HF_DATASETS_CACHE="$CACHE_ROOT/huggingface/datasets"
export TRANSFORMERS_CACHE="$CACHE_ROOT/huggingface/transformers"
export HF_HUB_CACHE="$CACHE_ROOT/huggingface/hub"

# PyTorch ç¼“å­˜
export TORCH_HOME="$CACHE_ROOT/torch"
export TORCH_EXTENSIONS_DIR="$CACHE_ROOT/torch/extensions"
export TORCHINDUCTOR_CACHE_DIR="$CACHE_ROOT/torch/inductor"

# Triton ç¼–è¯‘ç¼“å­˜
export TRITON_CACHE_DIR="$CACHE_ROOT/triton"

# Ray ç¼“å­˜å’Œä¸´æ—¶æ–‡ä»¶ï¼ˆä½¿ç”¨è¶…çŸ­è·¯å¾„é¿å… Unix socket 107 å­—èŠ‚é™åˆ¶ï¼‰
# export RAY_TMPDIR="/gpfs/projects/p32958/.r/tmp"
# export RAY_SESSION_DIR="/gpfs/projects/p32958/.r/session"  
# export RAY_LOG_DIR="/gpfs/projects/p32958/.r/logs"
export RAY_TMPDIR="/gpfs/projects/p32876/.r/tmp"
export RAY_SESSION_DIR="/gpfs/projects/p32876/.r/session"  
export RAY_LOG_DIR="/gpfs/projects/p32876/.r/logs"


# åˆ›å»ºRayç›®å½•
mkdir -p "$RAY_TMPDIR" "$RAY_SESSION_DIR" "$RAY_LOG_DIR"

# Python å­—èŠ‚ç ç¼“å­˜
export PYTHONPYCACHEPREFIX="$CACHE_ROOT/pycache"

# XDG ç¼“å­˜æ ‡å‡†
export XDG_CACHE_HOME="$CACHE_ROOT/xdg"

# é€šç”¨ä¸´æ—¶æ–‡ä»¶ç›®å½•
export TMPDIR="$CACHE_ROOT/tmp"
export TEMP="$CACHE_ROOT/tmp"
export TMP="$CACHE_ROOT/tmp"

# åˆ›å»ºæ‰€æœ‰ç›®å½•
mkdir -p "$HF_DATASETS_CACHE" "$TRANSFORMERS_CACHE" "$HF_HUB_CACHE"
mkdir -p "$TORCH_HOME" "$TORCH_EXTENSIONS_DIR" "$TORCHINDUCTOR_CACHE_DIR"
mkdir -p "$TRITON_CACHE_DIR"
mkdir -p "$RAY_TMPDIR" "$RAY_SESSION_DIR" "$RAY_LOG_DIR"
mkdir -p "$PYTHONPYCACHEPREFIX" "$XDG_CACHE_HOME" "$TMPDIR"

unset WANDB_RUN_ID
unset WANDB_RESUME

echo "WANDB ç¯å¢ƒå˜é‡ï¼š"
env | grep WANDB

# ===== ğŸŸ¢ è®­ç»ƒé…ç½® =====
# 7B æ¨¡å‹æ˜¾å­˜å ç”¨æ›´é«˜ï¼Œé€‚å½“å‡å° batch ç›¸å…³é…ç½®
# CHECKPOINT_DIR="/projects/p32958/Results/rl_ckpt/qwen25_vl_7b_rl/newest_35k_7b_${TIMESTAMP}"
CHECKPOINT_DIR="/projects/p32958/Results/rl_ckpt/qwen25_vl_7b_rl/newest_35k_7b_20251106-220335"

python3 -m verl.trainer.main \
  config=progresslm/configs/visual_demo_grpo.yaml \
  worker.actor.fsdp.torch_dtype=bfloat16 \
  worker.actor.model.model_path="${MODEL_PATH}" \
  worker.actor.model.tokenizer_path="${MODEL_PATH}" \
  worker.actor.global_batch_size=8 \
  data.rollout_batch_size=8 \
  worker.rollout.n=4 \
  worker.rollout.limit_images=24 \
  worker.rollout.max_num_batched_tokens=30000 \
  worker.rollout.gpu_memory_utilization=0.7 \
  trainer.save_checkpoint_path="${CHECKPOINT_DIR}" \
  trainer.experiment_name="qwen2_5vl7b_grpo_${TIMESTAMP}"
