#!/bin/bash

set -x

# ===== ğŸŸ¢ è·¯å¾„è®¾ç½® =====
MODEL_PATH="/projects/p32958/Results/full_model/qwen25vl_7b_sft"
DATA_FILE="/projects/p32958/chengxuan/ProgressLM/data/train/rl/new/new_rl_sampled_10k_ready_for_training.jsonl"

# å›ºå®šæ—¶é—´æˆ³ä»¥ä¾¿å¤ç°å®éªŒ
# TIMESTAMP=$(date +"%Y%m%d-%H%M%S")
TIMESTAMP="20251109-1942"

# ===== ğŸŸ¢ wandb è®¾ç½® =====
export WANDB_API_KEY="ac3c3d795e02ca8885235198ec9a222725622805"
export WANDB_PROJECT="progresslm_grpo_7b"
export WANDB_RUN_GROUP="qwen2_5_vl_7b_progresslm_grpo"
export WANDB_NAME="visual_demo_qwen2p5vl7b_10k_${TIMESTAMP}"
export WANDB_MODE="online"
export WANDB_DIR="/projects/p32876/Results/wandb_logs"

# ===== ğŸ”´ ç»Ÿä¸€ç¼“å­˜ç›®å½•è®¾ç½®ï¼ˆé¿å…ç£ç›˜é…é¢è¶…é™ï¼‰ =====
CACHE_ROOT="/gpfs/projects/p32876/chengxuan/.cache"

# HuggingFace ç¼“å­˜
export HF_HOME="$CACHE_ROOT/huggingface"
export HF_DATASETS_CACHE="$CACHE_ROOT/huggingface/datasets"
export TRANSFORMERS_CACHE="$CACHE_ROOT/huggingface/transformers"
export HF_HUB_CACHE="$CACHE_ROOT/huggingface/hub"

# PyTorch ç¼“å­˜
export TORCH_HOME="$CACHE_ROOT/torch"
export TORCH_EXTENSIONS_DIR="$CACHE_ROOT/torch/extensions"
export TORCHINDUCTOR_CACHE_DIR="$CACHE_ROOT/torch/inductor"

# Triton ç¼–è¯‘ç¼“å­˜
export TRITON_CACHE_DIR="$CACHE_ROOT/triton"

# Ray ç¼“å­˜å’Œä¸´æ—¶æ–‡ä»¶ï¼ˆä½¿ç”¨è¶…çŸ­è·¯å¾„é¿å… Unix socket 107 å­—èŠ‚é™åˆ¶ï¼‰
export RAY_TMPDIR="/gpfs/projects/p32876/.r/tmp"
export RAY_SESSION_DIR="/gpfs/projects/p32876/.r/session"
export RAY_LOG_DIR="/gpfs/projects/p32876/.r/logs"

# åˆ›å»ºRayç›®å½•
mkdir -p "$RAY_TMPDIR" "$RAY_SESSION_DIR" "$RAY_LOG_DIR"

# Python å­—èŠ‚ç ç¼“å­˜
export PYTHONPYCACHEPREFIX="$CACHE_ROOT/pycache"

# XDG ç¼“å­˜æ ‡å‡†
export XDG_CACHE_HOME="$CACHE_ROOT/xdg"

# é€šç”¨ä¸´æ—¶æ–‡ä»¶ç›®å½•
export TMPDIR="$CACHE_ROOT/tmp"
export TEMP="$CACHE_ROOT/tmp"
export TMP="$CACHE_ROOT/tmp"

# åˆ›å»ºæ‰€æœ‰ç›®å½•
mkdir -p "$HF_DATASETS_CACHE" "$TRANSFORMERS_CACHE" "$HF_HUB_CACHE"
mkdir -p "$TORCH_HOME" "$TORCH_EXTENSIONS_DIR" "$TORCHINDUCTOR_CACHE_DIR"
mkdir -p "$TRITON_CACHE_DIR"
mkdir -p "$RAY_TMPDIR" "$RAY_SESSION_DIR" "$RAY_LOG_DIR"
mkdir -p "$PYTHONPYCACHEPREFIX" "$XDG_CACHE_HOME" "$TMPDIR"

unset WANDB_RUN_ID
unset WANDB_RESUME

echo "WANDB ç¯å¢ƒå˜é‡ï¼š"
env | grep WANDB

# ===== ğŸŸ¢ è®­ç»ƒé…ç½® =====
CHECKPOINT_DIR="/projects/p32958/Results/rl_ckpt/qwen25_vl_7b_rl/newest_sampled_10k_7b_${TIMESTAMP}"

python3 -m verl.trainer.main \
  config=progresslm/configs/visual_demo_grpo.yaml \
  worker.actor.fsdp.torch_dtype=bfloat16 \
  worker.actor.model.model_path="${MODEL_PATH}" \
  worker.actor.model.tokenizer_path="${MODEL_PATH}" \
  worker.actor.global_batch_size=8 \
  data.rollout_batch_size=8 \
  data.train_files="${DATA_FILE}" \
  data.val_files="${DATA_FILE}" \
  worker.rollout.n=4 \
  worker.rollout.limit_images=24 \
  worker.rollout.max_num_batched_tokens=30000 \
  worker.rollout.gpu_memory_utilization=0.7 \
  trainer.save_checkpoint_path="${CHECKPOINT_DIR}" \
  trainer.experiment_name="qwen2_5vl7b_grpo_10k_${TIMESTAMP}"
