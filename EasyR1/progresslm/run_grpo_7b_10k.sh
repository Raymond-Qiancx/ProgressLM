#!/bin/bash

set -x

# ===== ğŸŸ¢ è·¯å¾„è®¾ç½® =====
MODEL_PATH="/projects/p32958/Results/full_model/qwen25vl_7b_sft"
DATA_FILE="/projects/p32958/chengxuan/ProgressLM/data/train/rl/new/new_rl_sampled_10k_ready_for_training.jsonl"

# å›ºå®šæ—¶é—´æˆ³ä»¥ä¾¿å¤ç°å®éªŒ
TIMESTAMP=$(date +"%Y%m%d-%H%M%S")
# TIMESTAMP="20251109-1942"

# ===== ğŸŸ¢ wandb è®¾ç½® =====
export WANDB_API_KEY="ac3c3d795e02ca8885235198ec9a222725622805"
export WANDB_PROJECT="progresslm_grpo_7b"
export WANDB_RUN_GROUP="qwen2_5_vl_7b_progresslm_grpo"
export WANDB_NAME="visual_demo_qwen2p5vl7b_10k_${TIMESTAMP}"
export WANDB_MODE="online"
export WANDB_DIR="/projects/p32876/Results/wandb_logs"

# ===== ğŸ”´ ç»Ÿä¸€ç¼“å­˜ç›®å½•è®¾ç½®ï¼ˆé¿å…ç£ç›˜é…é¢è¶…é™ï¼‰ =====
CACHE_ROOT="/gpfs/projects/p32876/chengxuan/.cache"

# HuggingFace ç¼“å­˜
export HF_HOME="$CACHE_ROOT/huggingface"
export HF_DATASETS_CACHE="$CACHE_ROOT/huggingface/datasets"
export TRANSFORMERS_CACHE="$CACHE_ROOT/huggingface/transformers"
export HF_HUB_CACHE="$CACHE_ROOT/huggingface/hub"

# PyTorch ç¼“å­˜
export TORCH_HOME="$CACHE_ROOT/torch"
export TORCH_EXTENSIONS_DIR="$CACHE_ROOT/torch/extensions"
export TORCHINDUCTOR_CACHE_DIR="$CACHE_ROOT/torch/inductor"

# Triton ç¼–è¯‘ç¼“å­˜
export TRITON_CACHE_DIR="$CACHE_ROOT/triton"

# Ray ç¼“å­˜å’Œä¸´æ—¶æ–‡ä»¶ï¼ˆä½¿ç”¨è¶…çŸ­è·¯å¾„é¿å… Unix socket 107 å­—èŠ‚é™åˆ¶ï¼‰
# ä½¿ç”¨è¶…çŸ­çš„ä¼šè¯ IDï¼ˆåªå–æ—¶é—´æˆ³å4ä½é¿å…è·¯å¾„è¿‡é•¿ï¼‰
SHORT_ID="${TIMESTAMP: -4}"  # åªå–æœ€å4ä½ï¼Œå¦‚ 1942
export RAY_TMPDIR="/gpfs/projects/p32876/.r/t${SHORT_ID}"
export RAY_SESSION_DIR="/gpfs/projects/p32876/.r/s${SHORT_ID}"
export RAY_LOG_DIR="/gpfs/projects/p32876/.r/logs"

# æ¸…ç†æ—§çš„ä¸´æ—¶æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
rm -rf "$RAY_TMPDIR" "$RAY_SESSION_DIR" 2>/dev/null

# åˆ›å»ºRayç›®å½•
mkdir -p "$RAY_TMPDIR" "$RAY_SESSION_DIR" "$RAY_LOG_DIR"

echo "Ray ä¼šè¯ ID: ${SHORT_ID}"
echo "Ray ä¸´æ—¶ç›®å½•: $RAY_TMPDIR"

# Python å­—èŠ‚ç ç¼“å­˜
export PYTHONPYCACHEPREFIX="$CACHE_ROOT/pycache"

# XDG ç¼“å­˜æ ‡å‡†
export XDG_CACHE_HOME="$CACHE_ROOT/xdg"

# é€šç”¨ä¸´æ—¶æ–‡ä»¶ç›®å½•
export TMPDIR="$CACHE_ROOT/tmp"
export TEMP="$CACHE_ROOT/tmp"
export TMP="$CACHE_ROOT/tmp"

# åˆ›å»ºæ‰€æœ‰ç›®å½•
mkdir -p "$HF_DATASETS_CACHE" "$TRANSFORMERS_CACHE" "$HF_HUB_CACHE"
mkdir -p "$TORCH_HOME" "$TORCH_EXTENSIONS_DIR" "$TORCHINDUCTOR_CACHE_DIR"
mkdir -p "$TRITON_CACHE_DIR"
mkdir -p "$RAY_TMPDIR" "$RAY_SESSION_DIR" "$RAY_LOG_DIR"
mkdir -p "$PYTHONPYCACHEPREFIX" "$XDG_CACHE_HOME" "$TMPDIR"

unset WANDB_RUN_ID
unset WANDB_RESUME

echo "WANDB ç¯å¢ƒå˜é‡ï¼š"
env | grep WANDB

# ===== ğŸŸ¢ è®­ç»ƒé…ç½® =====
CHECKPOINT_DIR="/projects/p32958/Results/rl_ckpt/qwen25_vl_7b_rl/newest_sampled_10k_7b_${TIMESTAMP}"

# ğŸ”§ æ¢¯åº¦ç´¯ç§¯é…ç½®è¯´æ˜:
# - global_batch_size=32 (è®­ç»ƒçš„æœ‰æ•ˆbatch sizeï¼ŒåŸæ¥æ˜¯8)
# - micro_batch_size_per_device_for_update=1 (è®­ç»ƒé˜¶æ®µæ¯GPUçš„ç‰©ç†batch)
# - micro_batch_size_per_device_for_experience=2 (ç»éªŒæ”¶é›†é˜¶æ®µæ¯GPUçš„ç‰©ç†batch)
# - rollout_batch_size=32 (å¿…é¡» >= global_batch_sizeï¼Œä¸”èƒ½è¢«å®ƒæ•´é™¤)
# - mini_rollout_batch_size=4 (DataLoaderæ¯æ¬¡è¯»å–çš„promptsæ•°)
#
# è®­ç»ƒé˜¶æ®µï¼š4 GPUs Ã— 1 æ ·æœ¬/GPU Ã— 8æ¬¡ç´¯ç§¯ = 32æ ·æœ¬æœ‰æ•ˆbatch
# Rollouté˜¶æ®µï¼šæ¯æ¬¡è¯»4ä¸ªprompts Ã— ç´¯ç§¯8æ¬¡ = 32ä¸ªprompts
# ç»éªŒæ”¶é›†ï¼š128ä¸ªå“åº” / (4 GPUs Ã— 2 æ ·æœ¬/GPU) = 16æ¬¡forward
# éªŒè¯ï¼š(32Ã—4) % 2 = 0 âœ“

python3 -m verl.trainer.main \
  config=progresslm/configs/visual_demo_grpo_7b.yaml \
  worker.actor.fsdp.torch_dtype=bfloat16 \
  worker.actor.model.model_path="${MODEL_PATH}" \
  worker.actor.model.tokenizer_path="${MODEL_PATH}" \
  worker.actor.global_batch_size=32 \
  worker.actor.micro_batch_size_per_device_for_update=1 \
  worker.actor.micro_batch_size_per_device_for_experience=1 \
  data.rollout_batch_size=32 \
  data.train_files="${DATA_FILE}" \
  data.val_files="${DATA_FILE}" \
  worker.rollout.n=4 \
  worker.rollout.limit_images=50 \
  worker.rollout.max_num_batched_tokens=20000 \
  worker.rollout.gpu_memory_utilization=0.4 \
  trainer.save_checkpoint_path="${CHECKPOINT_DIR}" \
  trainer.experiment_name="qwen2_5vl7b_grpo_10k_${TIMESTAMP}"
