{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8558c2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成！输出文件已保存到： /projects/b1222/userdata/jianshu/chengxuan/ProgressLM/data/eval/visual/visual_eval_all.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "input_path = \"/projects/b1222/userdata/jianshu/chengxuan/ProgressLM/data/eval/visual/visual_eval_all_1.jsonl\"\n",
    "output_path = \"/projects/b1222/userdata/jianshu/chengxuan/ProgressLM/data/eval/visual/visual_eval_all.jsonl\"\n",
    "\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as infile, \\\n",
    "     open(output_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for line in infile:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        data = json.loads(line)\n",
    "        if \"data_source\" in data and isinstance(data[\"data_source\"], str):\n",
    "            if data[\"data_source\"].startswith(\"robomind_\"):\n",
    "                data[\"data_source\"] = data[\"data_source\"].replace(\"robomind_\", \"\", 1)\n",
    "        json.dump(data, outfile, ensure_ascii=False)\n",
    "        outfile.write(\"\\n\")\n",
    "\n",
    "print(\"处理完成！输出文件已保存到：\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e40f3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "def parse_id(id_str: str) -> Tuple[str, str, str]:\n",
    "    \"\"\"解析id字段，返回(source, action_category, trajectory_id)\"\"\"\n",
    "    parts = id_str.split('/')\n",
    "    if len(parts) >= 3:\n",
    "        source = parts[0]\n",
    "        action_category = parts[1]\n",
    "        trajectory_id = parts[2].split('.')[0]  # 去掉可能的扩展名部分\n",
    "        return source, action_category, trajectory_id\n",
    "    return None, None, None\n",
    "\n",
    "def load_jsonl(filepath: str) -> List[Dict]:\n",
    "    \"\"\"加载JSONL文件\"\"\"\n",
    "    data = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "def save_jsonl(data: List[Dict], filepath: str):\n",
    "    \"\"\"保存为JSONL文件\"\"\"\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "def filter_and_sample(input_file: str, output_file: str, target_count: int = 3000):\n",
    "    \"\"\"\n",
    "    筛选和采样数据\n",
    "    \n",
    "    Args:\n",
    "        input_file: 输入的JSONL文件路径\n",
    "        output_file: 输出的JSONL文件路径\n",
    "        target_count: 目标采样数量（约3000）\n",
    "    \"\"\"\n",
    "    print(f\"开始读取数据...\")\n",
    "    all_data = load_jsonl(input_file)\n",
    "    print(f\"总共读取 {len(all_data)} 条数据\")\n",
    "    \n",
    "    # 步骤1: 筛选total_steps在3-7之间的数据\n",
    "    print(\"\\n步骤1: 筛选total_steps在3-7之间的数据...\")\n",
    "    filtered_data = []\n",
    "    for item in all_data:\n",
    "        total_steps = item.get('total_steps')\n",
    "        # 处理可能是字符串的情况\n",
    "        if isinstance(total_steps, str):\n",
    "            total_steps = int(total_steps)\n",
    "        if 3 <= total_steps <= 7:\n",
    "            filtered_data.append(item)\n",
    "    \n",
    "    print(f\"筛选后剩余 {len(filtered_data)} 条数据\")\n",
    "    \n",
    "    if len(filtered_data) == 0:\n",
    "        print(\"错误：没有符合条件的数据！\")\n",
    "        return\n",
    "    \n",
    "    # 步骤2: 按trajectory_id分组\n",
    "    print(\"\\n步骤2: 按trajectory_id分组...\")\n",
    "    trajectory_groups = defaultdict(list)\n",
    "    source_stats = defaultdict(int)\n",
    "    action_categories = set()\n",
    "    \n",
    "    for item in filtered_data:\n",
    "        source, action_category, trajectory_id = parse_id(item['id'])\n",
    "        if source and action_category and trajectory_id:\n",
    "            full_trajectory_id = f\"{source}/{action_category}/{trajectory_id}\"\n",
    "            trajectory_groups[full_trajectory_id].append(item)\n",
    "            source_stats[source] += 1\n",
    "            action_categories.add(f\"{source}/{action_category}\")\n",
    "    \n",
    "    print(f\"共有 {len(trajectory_groups)} 个不同的trajectory\")\n",
    "    print(f\"共有 {len(action_categories)} 个不同的action类别\")\n",
    "    \n",
    "    # 统计每个source的比例\n",
    "    total_filtered = len(filtered_data)\n",
    "    source_ratios = {source: count / total_filtered for source, count in source_stats.items()}\n",
    "    \n",
    "    print(\"\\n各data_source的数据分布:\")\n",
    "    for source, ratio in sorted(source_ratios.items()):\n",
    "        print(f\"  {source}: {source_stats[source]} 条 ({ratio*100:.2f}%)\")\n",
    "    \n",
    "    print(\"\\n各action类别:\")\n",
    "    for action in sorted(action_categories):\n",
    "        action_count = sum(1 for tid, items in trajectory_groups.items() if tid.startswith(action))\n",
    "        print(f\"  {action}: {action_count} trajectories\")\n",
    "    \n",
    "    # 步骤3: 分层采样\n",
    "    print(f\"\\n步骤3: 进行分层采样 (目标约{target_count}条)...\")\n",
    "    \n",
    "    # 为每个action类别至少保留一个trajectory\n",
    "    action_to_trajectories = defaultdict(list)\n",
    "    for trajectory_id, items in trajectory_groups.items():\n",
    "        parts = trajectory_id.split('/')\n",
    "        if len(parts) >= 2:\n",
    "            action_key = f\"{parts[0]}/{parts[1]}\"\n",
    "            action_to_trajectories[action_key].append(trajectory_id)\n",
    "    \n",
    "    # 确保每个action类别至少有一个trajectory\n",
    "    must_include_trajectories = set()\n",
    "    for action_key, trajectories in action_to_trajectories.items():\n",
    "        # 随机选择一个trajectory作为该类别的代表\n",
    "        must_include_trajectories.add(random.choice(trajectories))\n",
    "    \n",
    "    print(f\"为保证覆盖所有action类别，至少包含 {len(must_include_trajectories)} 个trajectory\")\n",
    "    \n",
    "    # 按source分组trajectories\n",
    "    source_to_trajectories = defaultdict(list)\n",
    "    for trajectory_id in trajectory_groups.keys():\n",
    "        source = trajectory_id.split('/')[0]\n",
    "        source_to_trajectories[source].append(trajectory_id)\n",
    "    \n",
    "    # 计算每个source应该采样多少个trajectory\n",
    "    selected_trajectories = set(must_include_trajectories)\n",
    "    \n",
    "    # 估算：如果平均每个trajectory有n条数据\n",
    "    avg_items_per_trajectory = len(filtered_data) / len(trajectory_groups)\n",
    "    print(f\"平均每个trajectory有 {avg_items_per_trajectory:.1f} 条数据\")\n",
    "    \n",
    "    # 计算还需要多少条数据\n",
    "    current_count = sum(len(trajectory_groups[tid]) for tid in selected_trajectories)\n",
    "    remaining_target = target_count - current_count\n",
    "    \n",
    "    print(f\"已选中 {len(selected_trajectories)} 个trajectory，共 {current_count} 条数据\")\n",
    "    print(f\"还需要约 {remaining_target} 条数据\")\n",
    "    \n",
    "    # 按比例从各个source采样剩余的trajectories\n",
    "    for source, trajectories in source_to_trajectories.items():\n",
    "        # 该source还有哪些trajectory未被选中\n",
    "        available = [t for t in trajectories if t not in selected_trajectories]\n",
    "        if not available:\n",
    "            continue\n",
    "        \n",
    "        # 计算该source应该采样的数量（按比例）\n",
    "        source_target = int(remaining_target * source_ratios[source])\n",
    "        # 转换为trajectory数量\n",
    "        needed_trajectories = max(1, int(source_target / avg_items_per_trajectory))\n",
    "        needed_trajectories = min(needed_trajectories, len(available))\n",
    "        \n",
    "        # 随机采样\n",
    "        sampled = random.sample(available, needed_trajectories)\n",
    "        selected_trajectories.update(sampled)\n",
    "    \n",
    "    # 收集所有选中trajectory的数据\n",
    "    sampled_data = []\n",
    "    for trajectory_id in selected_trajectories:\n",
    "        sampled_data.extend(trajectory_groups[trajectory_id])\n",
    "    \n",
    "    # 如果数据量还不够，继续补充\n",
    "    if len(sampled_data) < target_count * 0.9:  # 如果少于目标的90%\n",
    "        print(f\"\\n数据量不足，继续补充...\")\n",
    "        remaining_trajectories = [t for t in trajectory_groups.keys() if t not in selected_trajectories]\n",
    "        random.shuffle(remaining_trajectories)\n",
    "        \n",
    "        for trajectory_id in remaining_trajectories:\n",
    "            if len(sampled_data) >= target_count:\n",
    "                break\n",
    "            sampled_data.extend(trajectory_groups[trajectory_id])\n",
    "            selected_trajectories.add(trajectory_id)\n",
    "    \n",
    "    print(f\"\\n最终选中 {len(selected_trajectories)} 个trajectory，共 {len(sampled_data)} 条数据\")\n",
    "    \n",
    "    # 验证各source的比例\n",
    "    print(\"\\n最终各data_source的数据分布:\")\n",
    "    final_source_stats = defaultdict(int)\n",
    "    for item in sampled_data:\n",
    "        source = item.get('data_source')\n",
    "        final_source_stats[source] += 1\n",
    "    \n",
    "    for source in sorted(final_source_stats.keys()):\n",
    "        count = final_source_stats[source]\n",
    "        ratio = count / len(sampled_data)\n",
    "        original_ratio = source_ratios.get(source, 0)\n",
    "        print(f\"  {source}: {count} 条 ({ratio*100:.2f}%, 原始比例: {original_ratio*100:.2f}%)\")\n",
    "    \n",
    "    # 验证action类别覆盖\n",
    "    print(\"\\n最终action类别覆盖:\")\n",
    "    final_actions = set()\n",
    "    for item in sampled_data:\n",
    "        source, action_category, _ = parse_id(item['id'])\n",
    "        if source and action_category:\n",
    "            final_actions.add(f\"{source}/{action_category}\")\n",
    "    \n",
    "    print(f\"覆盖了 {len(final_actions)} / {len(action_categories)} 个action类别\")\n",
    "    missing_actions = action_categories - final_actions\n",
    "    if missing_actions:\n",
    "        print(f\"缺失的类别: {missing_actions}\")\n",
    "    else:\n",
    "        print(\"✓ 所有action类别都已覆盖\")\n",
    "    \n",
    "    # 保存结果\n",
    "    print(f\"\\n保存结果到 {output_file}...\")\n",
    "    save_jsonl(sampled_data, output_file)\n",
    "    print(\"完成！\")\n",
    "    \n",
    "    # 生成统计报告\n",
    "    report_file = output_file.replace('.jsonl', '_report.txt')\n",
    "    with open(report_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"数据采样报告\\n\")\n",
    "        f.write(f\"=\" * 50 + \"\\n\\n\")\n",
    "        f.write(f\"原始数据总量: {len(all_data)}\\n\")\n",
    "        f.write(f\"筛选后数据量: {len(filtered_data)}\\n\")\n",
    "        f.write(f\"采样后数据量: {len(sampled_data)}\\n\")\n",
    "        f.write(f\"选中trajectory数: {len(selected_trajectories)}\\n\\n\")\n",
    "        f.write(f\"各data_source分布:\\n\")\n",
    "        for source in sorted(final_source_stats.keys()):\n",
    "            count = final_source_stats[source]\n",
    "            ratio = count / len(sampled_data)\n",
    "            original_ratio = source_ratios.get(source, 0)\n",
    "            f.write(f\"  {source}: {count} 条 ({ratio*100:.2f}%, 原始: {original_ratio*100:.2f}%)\\n\")\n",
    "        f.write(f\"\\nAction类别覆盖: {len(final_actions)} / {len(action_categories)}\\n\")\n",
    "    \n",
    "    print(f\"统计报告已保存到 {report_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    \n",
    "    if len(sys.argv) < 2:\n",
    "        print(\"用法: python filter_jsonl.py <input_file> [output_file] [target_count]\")\n",
    "        print(\"示例: python filter_jsonl.py data.jsonl filtered_data.jsonl 3000\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    input_file = sys.argv[1]\n",
    "    output_file = sys.argv[2] if len(sys.argv) > 2 else \"filtered_output.jsonl\"\n",
    "    target_count = int(sys.argv[3]) if len(sys.argv) > 3 else 3000\n",
    "    \n",
    "    random.seed(42)  # 设置随机种子以便结果可复现\n",
    "    filter_and_sample(input_file, output_file, target_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen3vl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
