{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8276aebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“– è¯»å–JSONLæ–‡ä»¶: /home/runsheng/personal_3/qiancx/ProgressLM/data/raw/converted/h5_agilex_3rgb_converted.jsonl\n",
      "ğŸ“Š å‘ç° 2262 ä¸ªå”¯ä¸€IDéœ€è¦å¤„ç†\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "æå–å›¾åƒ: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2262/2262 [00:52<00:00, 42.85ID/s, æˆåŠŸ=2262, å¤±è´¥=0, å½“å‰=9_appleyellowplate_2...]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… å¤„ç†å®Œæˆï¼\n",
      "   æˆåŠŸ: 2262 ä¸ªID\n",
      "   å¤±è´¥: 0 ä¸ªID\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import h5py\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Set\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ImageExtractor:\n",
    "    \"\"\"ä»JSONLæ–‡ä»¶è¯»å–ä¿¡æ¯å¹¶ä»HDF5æ–‡ä»¶ä¸­æå–å¯¹åº”å›¾åƒ\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 base_data_path: str = \"/home/runsheng/personal_3/qiancx/Sources/datasets/robomind/benchmark1_0_compressed\",\n",
    "                 output_base_path: str = \"/home/runsheng/personal_3/qiancx/Sources/datasets/robomind/images\"):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–æå–å™¨\n",
    "        \n",
    "        Args:\n",
    "            base_data_path: HDF5æ•°æ®çš„åŸºç¡€è·¯å¾„\n",
    "            output_base_path: è¾“å‡ºå›¾åƒçš„åŸºç¡€è·¯å¾„\n",
    "        \"\"\"\n",
    "        self.base_data_path = Path(base_data_path)\n",
    "        self.output_base_path = Path(output_base_path)\n",
    "        self.errors = []  # è®°å½•é”™è¯¯\n",
    "        \n",
    "    def parse_id(self, id_string: str) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        è§£æIDå­—ç¬¦ä¸²\n",
    "        \n",
    "        Args:\n",
    "            id_string: æ ¼å¼å¦‚ \"h5_franka_3rgb/place_in_block_in_plate_1/1014_163457\"\n",
    "            \n",
    "        Returns:\n",
    "            åŒ…å«è§£æç»“æœçš„å­—å…¸\n",
    "        \"\"\"\n",
    "        parts = id_string.split('/')\n",
    "        if len(parts) < 3:\n",
    "            raise ValueError(f\"æ— æ•ˆçš„IDæ ¼å¼: {id_string}\")\n",
    "            \n",
    "        return {\n",
    "            'embodiment': parts[0],  # h5_franka_3rgb\n",
    "            'task_name': parts[1],    # place_in_block_in_plate_1\n",
    "            'episode_id': parts[2]     # 1014_163457\n",
    "        }\n",
    "    \n",
    "    def construct_h5_path(self, id_info: Dict[str, str]) -> Path:\n",
    "        \"\"\"\n",
    "        æ ¹æ®IDä¿¡æ¯æ„å»ºHDF5æ–‡ä»¶è·¯å¾„\n",
    "        \n",
    "        Args:\n",
    "            id_info: åŒ…å«embodiment, task_name, episode_idçš„å­—å…¸\n",
    "            \n",
    "        Returns:\n",
    "            HDF5æ–‡ä»¶çš„å®Œæ•´è·¯å¾„\n",
    "        \"\"\"\n",
    "        h5_dir = self.base_data_path / id_info['embodiment'] / id_info['task_name'] / \\\n",
    "                 'success_episodes' / 'train' / id_info['episode_id'] / 'data'\n",
    "        \n",
    "        # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨\n",
    "        if not h5_dir.exists():\n",
    "            raise FileNotFoundError(f\"ç›®å½•ä¸å­˜åœ¨: {h5_dir}\")\n",
    "        \n",
    "        # æŸ¥æ‰¾HDF5æ–‡ä»¶ï¼ˆé€šå¸¸åä¸ºtrajectory.hdf5ï¼‰\n",
    "        h5_files = list(h5_dir.glob('*.hdf5'))\n",
    "        if not h5_files:\n",
    "            raise FileNotFoundError(f\"æœªæ‰¾åˆ°HDF5æ–‡ä»¶åœ¨: {h5_dir}\")\n",
    "        \n",
    "        # ä¼˜å…ˆé€‰æ‹©trajectory.hdf5\n",
    "        for h5_file in h5_files:\n",
    "            if h5_file.name == 'trajectory.hdf5':\n",
    "                return h5_file\n",
    "        \n",
    "        return h5_files[0]  # å¦‚æœæ²¡æœ‰trajectory.hdf5ï¼Œè¿”å›ç¬¬ä¸€ä¸ªHDF5æ–‡ä»¶\n",
    "    \n",
    "    def decode_image(self, compressed_image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        è§£ç å‹ç¼©çš„å›¾åƒæ•°æ®\n",
    "        \n",
    "        Args:\n",
    "            compressed_image: å‹ç¼©çš„å›¾åƒæ•°æ®\n",
    "            \n",
    "        Returns:\n",
    "            è§£ç åçš„RGBå›¾åƒ\n",
    "        \"\"\"\n",
    "        if compressed_image is None:\n",
    "            return None\n",
    "            \n",
    "        # å¦‚æœæ˜¯å•ä¸ªå›¾åƒ\n",
    "        if len(compressed_image.shape) == 1:\n",
    "            rgb = cv2.imdecode(compressed_image, cv2.IMREAD_COLOR)\n",
    "            if rgb is None:\n",
    "                # å°è¯•ç›´æ¥è§£æä¸ºæ•°ç»„\n",
    "                rgb = np.frombuffer(compressed_image, dtype=np.uint8)\n",
    "                # æ ¹æ®å¤§å°åˆ¤æ–­å›¾åƒå°ºå¯¸\n",
    "                if rgb.size == 2764800:\n",
    "                    rgb = rgb.reshape(720, 1280, 3)\n",
    "                elif rgb.size == 921600:\n",
    "                    rgb = rgb.reshape(480, 640, 3)\n",
    "            return rgb\n",
    "        else:\n",
    "            # å¦‚æœæ˜¯æ‰¹é‡å›¾åƒï¼Œè¿”å›Noneï¼ˆéœ€è¦ç‰¹æ®Šå¤„ç†ï¼‰\n",
    "            return None\n",
    "    \n",
    "    def extract_specific_frames(self, h5_path: Path, frame_names: List[str]) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        ä»HDF5æ–‡ä»¶ä¸­æå–ç‰¹å®šå¸§ï¼ˆè‡ªåŠ¨ä»å¸§åè¯†åˆ«ç›¸æœºï¼‰\n",
    "        \n",
    "        Args:\n",
    "            h5_path: HDF5æ–‡ä»¶è·¯å¾„\n",
    "            frame_names: è¦æå–çš„å¸§åç§°åˆ—è¡¨ï¼ˆå¦‚ [\"camera_front_0000.jpg\", \"camera_front_0281.jpg\"]ï¼‰\n",
    "            \n",
    "        Returns:\n",
    "            å¸§åç§°åˆ°å›¾åƒæ•°ç»„çš„æ˜ å°„\n",
    "        \"\"\"\n",
    "        images = {}\n",
    "        \n",
    "        with h5py.File(h5_path, 'r') as f:\n",
    "            # æ£€æŸ¥æ˜¯å¦å‹ç¼©\n",
    "            is_compress = f.attrs.get('compress', True)\n",
    "            \n",
    "            # æ£€æŸ¥RGBå›¾åƒè·¯å¾„\n",
    "            if 'observations' not in f or 'rgb_images' not in f['observations']:\n",
    "                return images\n",
    "            \n",
    "            rgb_images_group = f['observations']['rgb_images']\n",
    "            \n",
    "            # è§£æå¸§åå¹¶æå–å›¾åƒ\n",
    "            for frame_name in frame_names:\n",
    "                try:\n",
    "                    # ä»å¸§åæå–ç›¸æœºåç§°å’Œç´¢å¼•\n",
    "                    # \"camera_front_0000.jpg\" -> camera=\"camera_front\", idx=0\n",
    "                    # \"camera_left_wrist_0281.jpg\" -> camera=\"camera_left_wrist\", idx=281\n",
    "                    parts = frame_name.split('_')\n",
    "                    frame_idx_str = parts[-1].split('.')[0]  # \"0000\"\n",
    "                    camera_name = '_'.join(parts[:-1])        # \"camera_front\"\n",
    "                    frame_idx = int(frame_idx_str)\n",
    "                    \n",
    "                    # æ£€æŸ¥ç›¸æœºæ˜¯å¦å­˜åœ¨\n",
    "                    if camera_name not in rgb_images_group:\n",
    "                        continue\n",
    "                    \n",
    "                    rgb_data = rgb_images_group[camera_name]\n",
    "                    \n",
    "                    # æ£€æŸ¥ç´¢å¼•æ˜¯å¦æœ‰æ•ˆ\n",
    "                    if frame_idx >= len(rgb_data):\n",
    "                        continue\n",
    "                    \n",
    "                    # æå–å›¾åƒ\n",
    "                    if is_compress:\n",
    "                        # è§£ç å‹ç¼©å›¾åƒ\n",
    "                        compressed_img = rgb_data[frame_idx]\n",
    "                        img = self.decode_image(compressed_img)\n",
    "                        if img is not None:\n",
    "                            # è½¬æ¢BGRåˆ°RGB\n",
    "                            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                            images[frame_name] = img\n",
    "                    else:\n",
    "                        # ç›´æ¥è¯»å–æœªå‹ç¼©å›¾åƒ\n",
    "                        images[frame_name] = rgb_data[frame_idx]\n",
    "                        \n",
    "                except (ValueError, IndexError) as e:\n",
    "                    continue\n",
    "        \n",
    "        return images\n",
    "    \n",
    "    def save_images(self, images: Dict[str, np.ndarray], output_dir: Path):\n",
    "        \"\"\"\n",
    "        ä¿å­˜å›¾åƒåˆ°æŒ‡å®šç›®å½•\n",
    "        \n",
    "        Args:\n",
    "            images: å¸§åç§°åˆ°å›¾åƒæ•°ç»„çš„æ˜ å°„\n",
    "            output_dir: è¾“å‡ºç›®å½•\n",
    "        \"\"\"\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        for frame_name, img in images.items():\n",
    "            output_path = output_dir / frame_name\n",
    "            # è½¬æ¢RGBå›BGRç”¨äºOpenCVä¿å­˜\n",
    "            img_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "            cv2.imwrite(str(output_path), img_bgr)\n",
    "    \n",
    "    def process_jsonl(self, jsonl_path: str, limit: int = None):\n",
    "        \"\"\"\n",
    "        å¤„ç†JSONLæ–‡ä»¶å¹¶æå–æ‰€æœ‰å›¾åƒ\n",
    "        \n",
    "        Args:\n",
    "            jsonl_path: JSONLæ–‡ä»¶è·¯å¾„\n",
    "            limit: é™åˆ¶å¤„ç†çš„è¡Œæ•°ï¼ˆç”¨äºæµ‹è¯•ï¼‰\n",
    "        \"\"\"\n",
    "        # é‡ç½®é”™è¯¯åˆ—è¡¨\n",
    "        self.errors = []\n",
    "        \n",
    "        # æ”¶é›†æ‰€æœ‰éœ€è¦æå–çš„æ•°æ®\n",
    "        extraction_tasks = defaultdict(lambda: defaultdict(set))\n",
    "        \n",
    "        print(f\"ğŸ“– è¯»å–JSONLæ–‡ä»¶: {jsonl_path}\")\n",
    "        \n",
    "        # è¯»å–JSONLæ–‡ä»¶\n",
    "        with open(jsonl_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            if limit:\n",
    "                lines = lines[:limit]\n",
    "                \n",
    "            for line_num, line in enumerate(lines, 1):\n",
    "                try:\n",
    "                    data = json.loads(line.strip())\n",
    "                    id_string = data['id']\n",
    "                    \n",
    "                    # æ”¶é›†æ‰€æœ‰éœ€è¦çš„å¸§\n",
    "                    all_frames = set()\n",
    "                    \n",
    "                    # visual_demoä¸­çš„å¸§\n",
    "                    if 'visual_demo' in data:\n",
    "                        all_frames.update(data['visual_demo'])\n",
    "                    \n",
    "                    # stage_to_estimateä¸­çš„å¸§\n",
    "                    if 'stage_to_estimate' in data:\n",
    "                        all_frames.update(data['stage_to_estimate'])\n",
    "                    \n",
    "                    # æŒ‰IDå’Œç›¸æœºåˆ†ç»„\n",
    "                    extraction_tasks[id_string]['frames'].update(all_frames)\n",
    "                    \n",
    "                except json.JSONDecodeError:\n",
    "                    self.errors.append(f\"ç¬¬ {line_num} è¡Œ: JSONè§£æé”™è¯¯\")\n",
    "                except Exception as e:\n",
    "                    self.errors.append(f\"ç¬¬ {line_num} è¡Œ: {str(e)}\")\n",
    "        \n",
    "        print(f\"ğŸ“Š å‘ç° {len(extraction_tasks)} ä¸ªå”¯ä¸€IDéœ€è¦å¤„ç†\\n\")\n",
    "        \n",
    "        # ç»Ÿè®¡æˆåŠŸå’Œå¤±è´¥\n",
    "        success_count = 0\n",
    "        failed_count = 0\n",
    "        \n",
    "        # ä½¿ç”¨tqdmå¤„ç†æ¯ä¸ªå”¯ä¸€çš„ID\n",
    "        with tqdm(total=len(extraction_tasks), desc=\"æå–å›¾åƒ\", unit=\"ID\") as pbar:\n",
    "            for id_string, task_info in extraction_tasks.items():\n",
    "                try:\n",
    "                    # è§£æID\n",
    "                    id_info = self.parse_id(id_string)\n",
    "                    \n",
    "                    # æ„å»ºHDF5è·¯å¾„\n",
    "                    h5_path = self.construct_h5_path(id_info)\n",
    "                    \n",
    "                    # æå–æ‰€æœ‰å¸§\n",
    "                    frames_to_extract = list(task_info['frames'])\n",
    "                    \n",
    "                    # ä»HDF5æå–å›¾åƒ\n",
    "                    images = self.extract_specific_frames(h5_path, frames_to_extract)\n",
    "                    \n",
    "                    if images:\n",
    "                        # æ„å»ºè¾“å‡ºè·¯å¾„\n",
    "                        output_dir = self.output_base_path / id_info['embodiment'] / \\\n",
    "                                    id_info['task_name'] / id_info['episode_id']\n",
    "                        \n",
    "                        # ä¿å­˜å›¾åƒ\n",
    "                        self.save_images(images, output_dir)\n",
    "                        success_count += 1\n",
    "                        \n",
    "                        # æ›´æ–°è¿›åº¦æ¡æè¿°\n",
    "                        pbar.set_postfix({\n",
    "                            'æˆåŠŸ': success_count, \n",
    "                            'å¤±è´¥': failed_count,\n",
    "                            'å½“å‰': f\"{id_info['task_name'][:20]}...\"\n",
    "                        })\n",
    "                    else:\n",
    "                        failed_count += 1\n",
    "                        self.errors.append(f\"ID {id_string}: æœªèƒ½æå–ä»»ä½•å›¾åƒ\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    failed_count += 1\n",
    "                    self.errors.append(f\"ID {id_string}: {str(e)}\")\n",
    "                    pbar.set_postfix({'æˆåŠŸ': success_count, 'å¤±è´¥': failed_count})\n",
    "                \n",
    "                pbar.update(1)\n",
    "        \n",
    "        # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡\n",
    "        print(f\"\\nâœ… å¤„ç†å®Œæˆï¼\")\n",
    "        print(f\"   æˆåŠŸ: {success_count} ä¸ªID\")\n",
    "        print(f\"   å¤±è´¥: {failed_count} ä¸ªID\")\n",
    "        \n",
    "        # å¦‚æœæœ‰é”™è¯¯ï¼Œè¯¢é—®æ˜¯å¦æŸ¥çœ‹\n",
    "        if self.errors:\n",
    "            print(f\"\\nâš ï¸ å‘ç° {len(self.errors)} ä¸ªé”™è¯¯\")\n",
    "            response = input(\"æ˜¯å¦æŸ¥çœ‹é”™è¯¯è¯¦æƒ…ï¼Ÿ(y/n): \")\n",
    "            if response.lower() == 'y':\n",
    "                print(\"\\né”™è¯¯åˆ—è¡¨:\")\n",
    "                for i, error in enumerate(self.errors, 1):\n",
    "                    print(f\"  {i}. {error}\")\n",
    "    \n",
    "    def diagnose_path(self, id_string: str):\n",
    "        \"\"\"\n",
    "        è¯Šæ–­è·¯å¾„é—®é¢˜ï¼Œå¸®åŠ©è°ƒè¯•\n",
    "        \n",
    "        Args:\n",
    "            id_string: IDå­—ç¬¦ä¸²\n",
    "        \"\"\"\n",
    "        print(f\"\\nğŸ” è¯Šæ–­ID: {id_string}\")\n",
    "        \n",
    "        try:\n",
    "            id_info = self.parse_id(id_string)\n",
    "            print(f\"  è§£æç»“æœ:\")\n",
    "            print(f\"    - Embodiment: {id_info['embodiment']}\")\n",
    "            print(f\"    - Task: {id_info['task_name']}\")\n",
    "            print(f\"    - Episode: {id_info['episode_id']}\")\n",
    "            \n",
    "            # æ£€æŸ¥embodimentç›®å½•\n",
    "            embodiment_path = self.base_data_path / id_info['embodiment']\n",
    "            print(f\"\\n  æ£€æŸ¥embodimentç›®å½•: {embodiment_path}\")\n",
    "            if embodiment_path.exists():\n",
    "                print(f\"    âœ“ ç›®å½•å­˜åœ¨\")\n",
    "                \n",
    "                # åˆ—å‡ºæ‰€æœ‰ä»»åŠ¡\n",
    "                task_dirs = sorted([d.name for d in embodiment_path.iterdir() if d.is_dir()])\n",
    "                print(f\"    ğŸ“ æ‰¾åˆ° {len(task_dirs)} ä¸ªä»»åŠ¡ç›®å½•\")\n",
    "                \n",
    "                # æŸ¥æ‰¾ç›¸ä¼¼çš„ä»»åŠ¡å\n",
    "                from difflib import get_close_matches\n",
    "                similar = get_close_matches(id_info['task_name'], task_dirs, n=3, cutoff=0.6)\n",
    "                if similar:\n",
    "                    print(f\"    ğŸ’¡ ç›¸ä¼¼çš„ä»»åŠ¡å: {similar}\")\n",
    "                \n",
    "                # æ£€æŸ¥ç¡®åˆ‡çš„ä»»åŠ¡ç›®å½•\n",
    "                task_path = embodiment_path / id_info['task_name']\n",
    "                if task_path.exists():\n",
    "                    print(f\"\\n  âœ“ ä»»åŠ¡ç›®å½•å­˜åœ¨: {task_path}\")\n",
    "                    \n",
    "                    # æ£€æŸ¥episodeè·¯å¾„\n",
    "                    episode_path = task_path / 'success_episodes' / 'train' / id_info['episode_id']\n",
    "                    if episode_path.exists():\n",
    "                        print(f\"  âœ“ Episodeç›®å½•å­˜åœ¨: {episode_path}\")\n",
    "                        \n",
    "                        # æ£€æŸ¥dataç›®å½•\n",
    "                        data_path = episode_path / 'data'\n",
    "                        if data_path.exists():\n",
    "                            print(f\"  âœ“ Dataç›®å½•å­˜åœ¨: {data_path}\")\n",
    "                            \n",
    "                            # åˆ—å‡ºæ‰€æœ‰æ–‡ä»¶\n",
    "                            files = list(data_path.iterdir())\n",
    "                            print(f\"  ğŸ“ ç›®å½•å†…å®¹:\")\n",
    "                            for f in files:\n",
    "                                print(f\"      - {f.name} ({f.stat().st_size / 1024 / 1024:.2f} MB)\")\n",
    "                            \n",
    "                            # æŸ¥æ‰¾HDF5æ–‡ä»¶\n",
    "                            h5_files = list(data_path.glob('*.hdf5'))\n",
    "                            if h5_files:\n",
    "                                print(f\"  âœ… æ‰¾åˆ°HDF5æ–‡ä»¶: {h5_files[0].name}\")\n",
    "                            else:\n",
    "                                print(f\"  âŒ æœªæ‰¾åˆ°HDF5æ–‡ä»¶\")\n",
    "                        else:\n",
    "                            print(f\"  âŒ Dataç›®å½•ä¸å­˜åœ¨\")\n",
    "                    else:\n",
    "                        print(f\"  âŒ Episodeç›®å½•ä¸å­˜åœ¨\")\n",
    "                        # åˆ—å‡ºå¯ç”¨çš„episodes\n",
    "                        train_path = task_path / 'success_episodes' / 'train'\n",
    "                        if train_path.exists():\n",
    "                            episodes = sorted([d.name for d in train_path.iterdir() if d.is_dir()])[:5]\n",
    "                            print(f\"  ğŸ’¡ å¯ç”¨çš„episodesç¤ºä¾‹: {episodes}\")\n",
    "                else:\n",
    "                    print(f\"  âŒ ä»»åŠ¡ç›®å½•ä¸å­˜åœ¨: {id_info['task_name']}\")\n",
    "                    print(f\"  ğŸ’¡ è¯·æ£€æŸ¥ä»»åŠ¡åæ˜¯å¦æ­£ç¡®\")\n",
    "            else:\n",
    "                print(f\"    âŒ Embodimentç›®å½•ä¸å­˜åœ¨\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ è¯Šæ–­å¤±è´¥: {e}\")\n",
    "    \n",
    "    def list_available_tasks(self, embodiment: str = \"h5_franka_3rgb\"):\n",
    "        \"\"\"\n",
    "        åˆ—å‡ºæŒ‡å®šembodimentä¸‹çš„æ‰€æœ‰å¯ç”¨ä»»åŠ¡\n",
    "        \n",
    "        Args:\n",
    "            embodiment: embodimentåç§°\n",
    "        \"\"\"\n",
    "        embodiment_path = self.base_data_path / embodiment\n",
    "        \n",
    "        if not embodiment_path.exists():\n",
    "            print(f\"âŒ Embodimentç›®å½•ä¸å­˜åœ¨: {embodiment_path}\")\n",
    "            return []\n",
    "        \n",
    "        tasks = []\n",
    "        for task_dir in sorted(embodiment_path.iterdir()):\n",
    "            if task_dir.is_dir():\n",
    "                # æ£€æŸ¥æ˜¯å¦æœ‰success_episodes/trainç›®å½•\n",
    "                train_path = task_dir / 'success_episodes' / 'train'\n",
    "                if train_path.exists():\n",
    "                    episodes = list(train_path.iterdir())\n",
    "                    if episodes:\n",
    "                        tasks.append({\n",
    "                            'name': task_dir.name,\n",
    "                            'episodes': len(episodes)\n",
    "                        })\n",
    "        \n",
    "        print(f\"\\nğŸ“‹ {embodiment} ä¸‹çš„å¯ç”¨ä»»åŠ¡:\")\n",
    "        print(f\"{'ä»»åŠ¡åç§°':<50} {'Episodesæ•°é‡':>15}\")\n",
    "        print(\"-\" * 65)\n",
    "        for task in tasks:\n",
    "            print(f\"{task['name']:<50} {task['episodes']:>15}\")\n",
    "        \n",
    "        return tasks\n",
    "\n",
    "\n",
    "# ========== ä½¿ç”¨ç¤ºä¾‹ ==========\n",
    "\n",
    "def main():\n",
    "    # åˆ›å»ºæå–å™¨\n",
    "    extractor = ImageExtractor(\n",
    "        base_data_path=\"/home/runsheng/personal_3/qiancx/Sources/datasets/robomind/benchmark1_0_compressed\",\n",
    "        output_base_path=\"/home/runsheng/personal_3/qiancx/Sources/datasets/robomind/images\"\n",
    "    )\n",
    "    \n",
    "    # å¤„ç†JSONLæ–‡ä»¶\n",
    "    jsonl_file = \"/home/runsheng/personal_3/qiancx/ProgressLM/data/raw/converted/h5_agilex_3rgb_converted.jsonl\"\n",
    "    extractor.process_jsonl(jsonl_file)\n",
    "    \n",
    "    # å¦‚æœéœ€è¦è¯Šæ–­ç‰¹å®šIDçš„é—®é¢˜ï¼Œå¯ä»¥ä½¿ç”¨ï¼š\n",
    "    # extractor.diagnose_path(\"h5_agilex_3rgb/10_packplate_2/2024_09_28-16_27_42-172863566445571200.00\")\n",
    "    \n",
    "    # åˆ—å‡ºå¯ç”¨ä»»åŠ¡ï¼š\n",
    "    # extractor.list_available_tasks(\"h5_agilex_3rgb\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ff5bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” æ¢ç´¢HDF5æ–‡ä»¶ç»“æ„: /home/vcj9002/jianshu/chengxuan/Data/robomind/data/benchmark1_0_compressed/h5_franka_3rgb/place_in_trash/success_episodes/train/0924_144150/data/trajectory.hdf5\n",
      "\n",
      "================================================================================\n",
      "ğŸ“‹ æ–‡ä»¶å±æ€§:\n",
      "  @compress = True\n",
      "  @sim = False\n",
      "\n",
      "ğŸ“„ language_distilbert (Dataset)\n",
      "   Shape: (1, 1, 768)\n",
      "   Dtype: float16\n",
      "ğŸ“„ language_raw (Dataset)\n",
      "   Shape: (1,)\n",
      "   Dtype: object\n",
      "ğŸ“ master/ (Group)\n",
      "ğŸ“„ master/joint_position (Dataset)\n",
      "   Shape: (200, 8)\n",
      "   Dtype: float64\n",
      "ğŸ“ observations/ (Group)\n",
      "ğŸ“ observations/depth_images/ (Group)\n",
      "ğŸ“„ observations/depth_images/camera_left (Dataset)\n",
      "   Shape: (200,)\n",
      "   Dtype: object\n",
      "ğŸ“„ observations/depth_images/camera_right (Dataset)\n",
      "   Shape: (200,)\n",
      "   Dtype: object\n",
      "ğŸ“„ observations/depth_images/camera_top (Dataset)\n",
      "   Shape: (200,)\n",
      "   Dtype: object\n",
      "ğŸ“ observations/rgb_images/ (Group)\n",
      "ğŸ“„ observations/rgb_images/camera_left (Dataset)\n",
      "   Shape: (200,)\n",
      "   Dtype: object\n",
      "ğŸ“„ observations/rgb_images/camera_right (Dataset)\n",
      "   Shape: (200,)\n",
      "   Dtype: object\n",
      "ğŸ“„ observations/rgb_images/camera_top (Dataset)\n",
      "   Shape: (200,)\n",
      "   Dtype: object\n",
      "ğŸ“ puppet/ (Group)\n",
      "ğŸ“„ puppet/end_effector (Dataset)\n",
      "   Shape: (200, 6)\n",
      "   Dtype: float64\n",
      "ğŸ“„ puppet/joint_position (Dataset)\n",
      "   Shape: (200, 8)\n",
      "   Dtype: float64\n",
      "\n",
      "================================================================================\n",
      "ğŸ¯ é‡ç‚¹æ£€æŸ¥: RGBå›¾åƒè·¯å¾„\n",
      "================================================================================\n",
      "\n",
      "âœ… æ‰¾åˆ°: observations/rgb_images\n",
      "   ç›¸æœºåˆ—è¡¨: ['camera_left', 'camera_right', 'camera_top']\n",
      "     - camera_left: shape=(200,), dtype=object\n",
      "     - camera_right: shape=(200,), dtype=object\n",
      "     - camera_top: shape=(200,), dtype=object\n",
      "âŒ æœªæ‰¾åˆ°: observations/images\n",
      "âŒ æœªæ‰¾åˆ°: rgb_images\n",
      "âŒ æœªæ‰¾åˆ°: images\n",
      "âŒ æœªæ‰¾åˆ°: obs/rgb_images\n",
      "âŒ æœªæ‰¾åˆ°: obs/images\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "h5_path = \"/home/vcj9002/jianshu/chengxuan/Data/robomind/data/benchmark1_0_compressed/h5_franka_3rgb/close_trash/success_episodes/train/0926_112252/data/trajectory.hdf5\"\n",
    "\n",
    "def explore_h5_structure(path, max_depth=4):\n",
    "    \"\"\"é€’å½’æ¢ç´¢HDF5æ–‡ä»¶ç»“æ„\"\"\"\n",
    "    \n",
    "    def print_structure(name, obj, depth=0):\n",
    "        if depth > max_depth:\n",
    "            return\n",
    "        \n",
    "        indent = \"  \" * depth\n",
    "        \n",
    "        if isinstance(obj, h5py.Group):\n",
    "            print(f\"{indent}ğŸ“ {name}/ (Group)\")\n",
    "            # æ‰“å°å±æ€§\n",
    "            if obj.attrs:\n",
    "                for key, val in obj.attrs.items():\n",
    "                    print(f\"{indent}   @{key} = {val}\")\n",
    "        \n",
    "        elif isinstance(obj, h5py.Dataset):\n",
    "            print(f\"{indent}ğŸ“„ {name} (Dataset)\")\n",
    "            print(f\"{indent}   Shape: {obj.shape}\")\n",
    "            print(f\"{indent}   Dtype: {obj.dtype}\")\n",
    "            if obj.attrs:\n",
    "                for key, val in obj.attrs.items():\n",
    "                    print(f\"{indent}   @{key} = {val}\")\n",
    "    \n",
    "    print(f\"\\nğŸ” æ¢ç´¢HDF5æ–‡ä»¶ç»“æ„: {path}\\n\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    with h5py.File(path, 'r') as f:\n",
    "        # æ‰“å°æ–‡ä»¶çº§åˆ«å±æ€§\n",
    "        print(\"ğŸ“‹ æ–‡ä»¶å±æ€§:\")\n",
    "        for key, val in f.attrs.items():\n",
    "            print(f\"  @{key} = {val}\")\n",
    "        print()\n",
    "        \n",
    "        # é€’å½’æ‰“å°ç»“æ„\n",
    "        f.visititems(print_structure)\n",
    "        \n",
    "        # ç‰¹åˆ«å…³æ³¨rgb_images\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"ğŸ¯ é‡ç‚¹æ£€æŸ¥: RGBå›¾åƒè·¯å¾„\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # å°è¯•å¸¸è§è·¯å¾„\n",
    "        paths_to_check = [\n",
    "            'observations/rgb_images',\n",
    "            'observations/images',\n",
    "            'rgb_images',\n",
    "            'images',\n",
    "            'obs/rgb_images',\n",
    "            'obs/images',\n",
    "        ]\n",
    "        \n",
    "        for path_str in paths_to_check:\n",
    "            if path_str in f:\n",
    "                print(f\"\\nâœ… æ‰¾åˆ°: {path_str}\")\n",
    "                obj = f[path_str]\n",
    "                if isinstance(obj, h5py.Group):\n",
    "                    print(f\"   ç›¸æœºåˆ—è¡¨: {list(obj.keys())}\")\n",
    "                    for camera in obj.keys():\n",
    "                        print(f\"     - {camera}: shape={obj[camera].shape}, dtype={obj[camera].dtype}\")\n",
    "                else:\n",
    "                    print(f\"   Shape: {obj.shape}, Dtype: {obj.dtype}\")\n",
    "            else:\n",
    "                print(f\"âŒ æœªæ‰¾åˆ°: {path_str}\")\n",
    "\n",
    "# è¿è¡Œæ¢ç´¢\n",
    "explore_h5_structure(h5_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "561bc671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸš€ RGBå›¾åƒæ‰¹é‡æå–å·¥å…·\n",
      "================================================================================\n",
      "ğŸ“‚ è¾“å…¥ç›®å½•: /home/vcj9002/jianshu/chengxuan/Data/robomind/data/benchmark1_0_compressed\n",
      "ğŸ’¾ è¾“å‡ºç›®å½•: /home/vcj9002/jianshu/chengxuan/Data/robomind/data/3rgb\n",
      "================================================================================\n",
      "\n",
      "ğŸ” æ‰«æç›®å½•: /home/vcj9002/jianshu/chengxuan/Data/robomind/data/benchmark1_0_compressed\n",
      "ğŸ“Š æ‰¾åˆ° 12460 ä¸ªHDF5æ–‡ä»¶\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "æå–å›¾åƒ:   1%|          | 135/12460 [03:51<5:52:39,  1.72s/æ–‡ä»¶, æˆåŠŸ=135, å¤±è´¥=0, å›¾åƒ=50139]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 325\u001b[39m\n\u001b[32m    321\u001b[39m     extractor.extract_all(limit=args.limit)\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 321\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    320\u001b[39m \u001b[38;5;66;03m# æ‰§è¡Œæå–\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m \u001b[43mextractor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 221\u001b[39m, in \u001b[36mRGBImageExtractor.extract_all\u001b[39m\u001b[34m(self, limit)\u001b[39m\n\u001b[32m    217\u001b[39m images_by_camera = \u001b[38;5;28mself\u001b[39m.extract_images_from_h5(h5_path)\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m images_by_camera:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# ä¿å­˜å›¾åƒ\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msave_images_by_camera\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages_by_camera\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelative_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    222\u001b[39m     \u001b[38;5;28mself\u001b[39m.stats[\u001b[33m'\u001b[39m\u001b[33msuccessful_h5_files\u001b[39m\u001b[33m'\u001b[39m] += \u001b[32m1\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 132\u001b[39m, in \u001b[36mRGBImageExtractor.save_images_by_camera\u001b[39m\u001b[34m(self, images_by_camera, relative_path)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;66;03m# è½¬æ¢RGBå›BGRç”¨äºOpenCVä¿å­˜\u001b[39;00m\n\u001b[32m    131\u001b[39m img_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_bgr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[38;5;66;03m# æ›´æ–°ç»Ÿè®¡\u001b[39;00m\n\u001b[32m    135\u001b[39m \u001b[38;5;28mself\u001b[39m.stats[\u001b[33m'\u001b[39m\u001b[33mtotal_images\u001b[39m\u001b[33m'\u001b[39m] += \u001b[32m1\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "\n",
    "class RGBImageExtractor:\n",
    "    \"\"\"é€šç”¨RGBå›¾åƒæå–å™¨ - ä»HDF5æ–‡ä»¶æ‰¹é‡æå–æ‰€æœ‰RGBå›¾åƒ\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 base_data_path: str,\n",
    "                 output_base_path: str = \"/home/vcj9002/jianshu/chengxuan/Data/robomind/data/3rgb\"):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–æå–å™¨\n",
    "        \n",
    "        Args:\n",
    "            base_data_path: HDF5æ•°æ®çš„åŸºç¡€è·¯å¾„\n",
    "            output_base_path: è¾“å‡ºå›¾åƒçš„åŸºç¡€è·¯å¾„ï¼ˆæŒ‰ç›¸æœºåˆ†ç±»ï¼‰\n",
    "        \"\"\"\n",
    "        self.base_data_path = Path(base_data_path)\n",
    "        self.output_base_path = Path(output_base_path)\n",
    "        self.stats = {\n",
    "            'total_h5_files': 0,\n",
    "            'successful_h5_files': 0,\n",
    "            'failed_h5_files': 0,\n",
    "            'total_images': 0,\n",
    "            'cameras': {}\n",
    "        }\n",
    "        self.errors = []\n",
    "        \n",
    "    def decode_image(self, compressed_image: np.ndarray) -> Optional[np.ndarray]:\n",
    "        \"\"\"\n",
    "        è§£ç å‹ç¼©çš„å›¾åƒæ•°æ®\n",
    "        \n",
    "        Args:\n",
    "            compressed_image: å‹ç¼©çš„å›¾åƒæ•°æ®\n",
    "            \n",
    "        Returns:\n",
    "            è§£ç åçš„RGBå›¾åƒï¼ˆå·²è½¬æ¢ä¸ºRGBæ ¼å¼ï¼‰\n",
    "        \"\"\"\n",
    "        if compressed_image is None or len(compressed_image) == 0:\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            # ä½¿ç”¨cv2è§£ç \n",
    "            rgb = cv2.imdecode(compressed_image, cv2.IMREAD_COLOR)\n",
    "            if rgb is None:\n",
    "                return None\n",
    "            # è½¬æ¢BGRåˆ°RGB\n",
    "            rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)\n",
    "            return rgb\n",
    "        except Exception as e:\n",
    "            return None\n",
    "    \n",
    "    def extract_images_from_h5(self, h5_path: Path) -> Dict[str, List[np.ndarray]]:\n",
    "        \"\"\"\n",
    "        ä»å•ä¸ªHDF5æ–‡ä»¶ä¸­æå–æ‰€æœ‰RGBå›¾åƒ\n",
    "        \n",
    "        Args:\n",
    "            h5_path: HDF5æ–‡ä»¶è·¯å¾„\n",
    "            \n",
    "        Returns:\n",
    "            ç›¸æœºåç§°åˆ°å›¾åƒåˆ—è¡¨çš„æ˜ å°„\n",
    "        \"\"\"\n",
    "        images_by_camera = {}\n",
    "        \n",
    "        try:\n",
    "            with h5py.File(h5_path, 'r') as f:\n",
    "                # æ£€æŸ¥æ˜¯å¦æœ‰å‹ç¼©æ ‡å¿—\n",
    "                is_compress = f.attrs.get('compress', True)\n",
    "                \n",
    "                # æ£€æŸ¥RGBå›¾åƒè·¯å¾„\n",
    "                if 'observations' not in f or 'rgb_images' not in f['observations']:\n",
    "                    return images_by_camera\n",
    "                \n",
    "                rgb_images_group = f['observations']['rgb_images']\n",
    "                \n",
    "                # éå†æ‰€æœ‰ç›¸æœº\n",
    "                for camera_name in rgb_images_group.keys():\n",
    "                    camera_data = rgb_images_group[camera_name]\n",
    "                    images = []\n",
    "                    \n",
    "                    # æå–è¯¥ç›¸æœºçš„æ‰€æœ‰å¸§\n",
    "                    for frame_idx in range(len(camera_data)):\n",
    "                        try:\n",
    "                            if is_compress:\n",
    "                                # è§£ç å‹ç¼©å›¾åƒ\n",
    "                                compressed_img = camera_data[frame_idx]\n",
    "                                img = self.decode_image(compressed_img)\n",
    "                                if img is not None:\n",
    "                                    images.append(img)\n",
    "                            else:\n",
    "                                # ç›´æ¥è¯»å–æœªå‹ç¼©å›¾åƒ\n",
    "                                images.append(camera_data[frame_idx])\n",
    "                        except Exception as e:\n",
    "                            continue\n",
    "                    \n",
    "                    if images:\n",
    "                        images_by_camera[camera_name] = images\n",
    "                        \n",
    "        except Exception as e:\n",
    "            self.errors.append(f\"è¯»å– {h5_path}: {str(e)}\")\n",
    "            \n",
    "        return images_by_camera\n",
    "    \n",
    "    def save_images_by_camera(self, \n",
    "                             images_by_camera: Dict[str, List[np.ndarray]], \n",
    "                             relative_path: Path):\n",
    "        \"\"\"\n",
    "        æŒ‰ç›¸æœºåˆ†ç±»ä¿å­˜å›¾åƒ\n",
    "        \n",
    "        Args:\n",
    "            images_by_camera: ç›¸æœºåç§°åˆ°å›¾åƒåˆ—è¡¨çš„æ˜ å°„\n",
    "            relative_path: ç›¸å¯¹è·¯å¾„ï¼ˆembodiment/task/episodeï¼‰\n",
    "        \"\"\"\n",
    "        for camera_name, images in images_by_camera.items():\n",
    "            # æ„å»ºè¾“å‡ºç›®å½•: output_base/camera_name/embodiment/task/episode\n",
    "            camera_output_dir = self.output_base_path / camera_name / relative_path\n",
    "            camera_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # ä¿å­˜æ‰€æœ‰å¸§\n",
    "            for idx, img in enumerate(images):\n",
    "                # æ–‡ä»¶åæ ¼å¼: frame_0000.jpg, frame_0001.jpg, ...\n",
    "                frame_name = f\"frame_{idx:04d}.jpg\"\n",
    "                output_path = camera_output_dir / frame_name\n",
    "                \n",
    "                # è½¬æ¢RGBå›BGRç”¨äºOpenCVä¿å­˜\n",
    "                img_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "                cv2.imwrite(str(output_path), img_bgr)\n",
    "                \n",
    "                # æ›´æ–°ç»Ÿè®¡\n",
    "                self.stats['total_images'] += 1\n",
    "                if camera_name not in self.stats['cameras']:\n",
    "                    self.stats['cameras'][camera_name] = 0\n",
    "                self.stats['cameras'][camera_name] += 1\n",
    "    \n",
    "    def find_all_h5_files(self) -> List[tuple]:\n",
    "        \"\"\"\n",
    "        æŸ¥æ‰¾æ‰€æœ‰HDF5æ–‡ä»¶\n",
    "        \n",
    "        Returns:\n",
    "            (h5æ–‡ä»¶è·¯å¾„, ç›¸å¯¹è·¯å¾„)çš„åˆ—è¡¨\n",
    "        \"\"\"\n",
    "        h5_files = []\n",
    "        \n",
    "        print(f\"ğŸ” æ‰«æç›®å½•: {self.base_data_path}\")\n",
    "        \n",
    "        # éå†æ‰€æœ‰embodimentç›®å½•\n",
    "        for embodiment_dir in self.base_data_path.iterdir():\n",
    "            if not embodiment_dir.is_dir():\n",
    "                continue\n",
    "                \n",
    "            embodiment_name = embodiment_dir.name\n",
    "            \n",
    "            # éå†æ‰€æœ‰taskç›®å½•\n",
    "            for task_dir in embodiment_dir.iterdir():\n",
    "                if not task_dir.is_dir():\n",
    "                    continue\n",
    "                    \n",
    "                task_name = task_dir.name\n",
    "                \n",
    "                # æŸ¥æ‰¾success_episodes/trainä¸‹çš„æ‰€æœ‰episode\n",
    "                train_dir = task_dir / 'success_episodes' / 'train'\n",
    "                if not train_dir.exists():\n",
    "                    continue\n",
    "                \n",
    "                for episode_dir in train_dir.iterdir():\n",
    "                    if not episode_dir.is_dir():\n",
    "                        continue\n",
    "                        \n",
    "                    episode_name = episode_dir.name\n",
    "                    \n",
    "                    # æŸ¥æ‰¾dataç›®å½•ä¸‹çš„HDF5æ–‡ä»¶\n",
    "                    data_dir = episode_dir / 'data'\n",
    "                    if not data_dir.exists():\n",
    "                        continue\n",
    "                    \n",
    "                    # æŸ¥æ‰¾æ‰€æœ‰.hdf5æ–‡ä»¶\n",
    "                    for h5_file in data_dir.glob('*.hdf5'):\n",
    "                        # æ„å»ºç›¸å¯¹è·¯å¾„\n",
    "                        relative_path = Path(embodiment_name) / task_name / episode_name\n",
    "                        h5_files.append((h5_file, relative_path))\n",
    "        \n",
    "        return h5_files\n",
    "    \n",
    "    def extract_all(self, limit: Optional[int] = None):\n",
    "        \"\"\"\n",
    "        æå–æ‰€æœ‰RGBå›¾åƒ\n",
    "        \n",
    "        Args:\n",
    "            limit: é™åˆ¶å¤„ç†çš„HDF5æ–‡ä»¶æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰\n",
    "        \"\"\"\n",
    "        # æŸ¥æ‰¾æ‰€æœ‰HDF5æ–‡ä»¶\n",
    "        h5_files = self.find_all_h5_files()\n",
    "        \n",
    "        if not h5_files:\n",
    "            print(\"âŒ æœªæ‰¾åˆ°ä»»ä½•HDF5æ–‡ä»¶\")\n",
    "            return\n",
    "        \n",
    "        print(f\"ğŸ“Š æ‰¾åˆ° {len(h5_files)} ä¸ªHDF5æ–‡ä»¶\\n\")\n",
    "        \n",
    "        # é™åˆ¶å¤„ç†æ•°é‡ï¼ˆå¦‚æœæŒ‡å®šï¼‰\n",
    "        if limit:\n",
    "            h5_files = h5_files[:limit]\n",
    "            print(f\"âš ï¸  é™åˆ¶å¤„ç†å‰ {limit} ä¸ªæ–‡ä»¶\\n\")\n",
    "        \n",
    "        self.stats['total_h5_files'] = len(h5_files)\n",
    "        \n",
    "        # ä½¿ç”¨tqdmå¤„ç†æ¯ä¸ªHDF5æ–‡ä»¶\n",
    "        with tqdm(total=len(h5_files), desc=\"æå–å›¾åƒ\", unit=\"æ–‡ä»¶\") as pbar:\n",
    "            for h5_path, relative_path in h5_files:\n",
    "                try:\n",
    "                    # æå–å›¾åƒ\n",
    "                    images_by_camera = self.extract_images_from_h5(h5_path)\n",
    "                    \n",
    "                    if images_by_camera:\n",
    "                        # ä¿å­˜å›¾åƒ\n",
    "                        self.save_images_by_camera(images_by_camera, relative_path)\n",
    "                        self.stats['successful_h5_files'] += 1\n",
    "                    else:\n",
    "                        self.stats['failed_h5_files'] += 1\n",
    "                        self.errors.append(f\"æ— å›¾åƒ: {relative_path}\")\n",
    "                    \n",
    "                    # æ›´æ–°è¿›åº¦æ¡\n",
    "                    pbar.set_postfix({\n",
    "                        'æˆåŠŸ': self.stats['successful_h5_files'],\n",
    "                        'å¤±è´¥': self.stats['failed_h5_files'],\n",
    "                        'å›¾åƒ': self.stats['total_images']\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    self.stats['failed_h5_files'] += 1\n",
    "                    self.errors.append(f\"{relative_path}: {str(e)}\")\n",
    "                    pbar.set_postfix({\n",
    "                        'æˆåŠŸ': self.stats['successful_h5_files'],\n",
    "                        'å¤±è´¥': self.stats['failed_h5_files']\n",
    "                    })\n",
    "                \n",
    "                pbar.update(1)\n",
    "        \n",
    "        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯\n",
    "        self.print_summary()\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"æ‰“å°æå–ç»Ÿè®¡ä¿¡æ¯\"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"âœ… æå–å®Œæˆï¼\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:\")\n",
    "        print(f\"   æ€»HDF5æ–‡ä»¶æ•°: {self.stats['total_h5_files']}\")\n",
    "        print(f\"   æˆåŠŸå¤„ç†: {self.stats['successful_h5_files']}\")\n",
    "        print(f\"   å¤„ç†å¤±è´¥: {self.stats['failed_h5_files']}\")\n",
    "        print(f\"   æå–å›¾åƒæ€»æ•°: {self.stats['total_images']}\")\n",
    "        \n",
    "        if self.stats['cameras']:\n",
    "            print(f\"\\nğŸ“· æŒ‰ç›¸æœºç»Ÿè®¡:\")\n",
    "            for camera, count in sorted(self.stats['cameras'].items()):\n",
    "                print(f\"   {camera}: {count} å¼ \")\n",
    "        \n",
    "        print(f\"\\nğŸ’¾ è¾“å‡ºç›®å½•: {self.output_base_path}\")\n",
    "        \n",
    "        # æ˜¾ç¤ºç›®å½•ç»“æ„ç¤ºä¾‹\n",
    "        print(f\"\\nğŸ“ è¾“å‡ºç›®å½•ç»“æ„:\")\n",
    "        print(f\"   {self.output_base_path}/\")\n",
    "        if self.stats['cameras']:\n",
    "            for camera in sorted(self.stats['cameras'].keys()):\n",
    "                print(f\"   â”œâ”€â”€ {camera}/\")\n",
    "                print(f\"   â”‚   â”œâ”€â”€ embodiment_name/\")\n",
    "                print(f\"   â”‚   â”‚   â”œâ”€â”€ task_name/\")\n",
    "                print(f\"   â”‚   â”‚   â”‚   â”œâ”€â”€ episode_name/\")\n",
    "                print(f\"   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ frame_0000.jpg\")\n",
    "                print(f\"   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ frame_0001.jpg\")\n",
    "                print(f\"   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ...\")\n",
    "                break  # åªæ˜¾ç¤ºä¸€ä¸ªç¤ºä¾‹\n",
    "        \n",
    "        # æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯\n",
    "        if self.errors:\n",
    "            print(f\"\\nâš ï¸  å‘ç° {len(self.errors)} ä¸ªé”™è¯¯\")\n",
    "            response = input(\"æ˜¯å¦æŸ¥çœ‹é”™è¯¯è¯¦æƒ…ï¼Ÿ(y/n): \")\n",
    "            if response.lower() == 'y':\n",
    "                print(\"\\né”™è¯¯åˆ—è¡¨:\")\n",
    "                for i, error in enumerate(self.errors[:50], 1):  # æœ€å¤šæ˜¾ç¤º50ä¸ª\n",
    "                    print(f\"  {i}. {error}\")\n",
    "                if len(self.errors) > 50:\n",
    "                    print(f\"  ... è¿˜æœ‰ {len(self.errors) - 50} ä¸ªé”™è¯¯æœªæ˜¾ç¤º\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='ä»HDF5æ–‡ä»¶æ‰¹é‡æå–RGBå›¾åƒ')\n",
    "    parser.add_argument('--base_path', type=str, \n",
    "                       default='/home/vcj9002/jianshu/chengxuan/Data/robomind/data/benchmark1_0_compressed',\n",
    "                       help='HDF5æ•°æ®çš„åŸºç¡€è·¯å¾„')\n",
    "    parser.add_argument('--output_path', type=str,\n",
    "                       default='/home/vcj9002/jianshu/chengxuan/Data/robomind/data/3rgb',\n",
    "                       help='è¾“å‡ºå›¾åƒçš„åŸºç¡€è·¯å¾„')\n",
    "    parser.add_argument('--limit', type=int, default=None,\n",
    "                       help='é™åˆ¶å¤„ç†çš„æ–‡ä»¶æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰')\n",
    "    \n",
    "    # ä½¿ç”¨parse_known_args()ä»¥æ”¯æŒJupyter notebookç¯å¢ƒ\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    \n",
    "    # åˆ›å»ºæå–å™¨\n",
    "    extractor = RGBImageExtractor(\n",
    "        base_data_path=args.base_path,\n",
    "        output_base_path=args.output_path\n",
    "    )\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"ğŸš€ RGBå›¾åƒæ‰¹é‡æå–å·¥å…·\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ğŸ“‚ è¾“å…¥ç›®å½•: {args.base_path}\")\n",
    "    print(f\"ğŸ’¾ è¾“å‡ºç›®å½•: {args.output_path}\")\n",
    "    if args.limit:\n",
    "        print(f\"âš ï¸  æµ‹è¯•æ¨¡å¼: åªå¤„ç†å‰ {args.limit} ä¸ªæ–‡ä»¶\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # æ‰§è¡Œæå–\n",
    "    extractor.extract_all(limit=args.limit)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bf5b257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸš€ RGBå›¾åƒæ‰¹é‡æå–ï¼ˆå¤šçº¿ç¨‹ç‰ˆï¼‰\n",
      "================================================================================\n",
      "ğŸ“‚ è¾“å…¥: /home/vcj9002/jianshu/chengxuan/Data/robomind/data/benchmark1_0_compressed\n",
      "ğŸ’¾ è¾“å‡º: /home/vcj9002/jianshu/chengxuan/Data/robomind/data/3rgb\n",
      "ğŸ§µ çº¿ç¨‹: 128\n",
      "================================================================================\n",
      "\n",
      "ğŸ” æ‰«æç›®å½•: /home/vcj9002/jianshu/chengxuan/Data/robomind/data/benchmark1_0_compressed\n",
      "ğŸ“Š æ‰¾åˆ° 12460 ä¸ªHDF5æ–‡ä»¶\n",
      "ğŸ”§ ä½¿ç”¨ 128 ä¸ªçº¿ç¨‹å¹¶è¡Œå¤„ç†\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "æå–å›¾åƒ:   6%|â–Œ         | 724/12460 [08:50<6:32:59,  2.01s/æ–‡ä»¶, æˆåŠŸ=624, å¤±è´¥=100, å›¾åƒ=274677][ERROR:169@537.842] global loadsave.cpp:1324 imdecode_ imdecode_(''): can't read header: OpenCV(4.12.0) /io/opencv/modules/imgcodecs/src/grfmt_bmp.cpp:99: error: (-215:Assertion failed) size > 0 in function 'readHeader'\n",
      "\n",
      "æå–å›¾åƒ:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 8870/12460 [1:51:48<56:29,  1.06æ–‡ä»¶/s, æˆåŠŸ=7373, å¤±è´¥=1497, å›¾åƒ=3074079]  [ERROR:0@6709.028] global loadsave.cpp:1324 imdecode_ imdecode_(''): can't read header: OpenCV(4.12.0) /io/opencv/modules/imgcodecs/src/grfmt_bmp.cpp:108: error: (-215:Assertion failed) m_rle_code_ >= 0 && m_rle_code_ <= BMP_BITFIELDS in function 'readHeader'\n",
      "\n",
      "æå–å›¾åƒ:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 8915/12460 [1:52:50<50:56,  1.16æ–‡ä»¶/s, æˆåŠŸ=7388, å¤±è´¥=1527, å›¾åƒ=3080628]  [ERROR:111@6771.400] global loadsave.cpp:1324 imdecode_ imdecode_(''): can't read header: OpenCV(4.12.0) /io/opencv/modules/imgcodecs/src/grfmt_bmp.cpp:108: error: (-215:Assertion failed) m_rle_code_ >= 0 && m_rle_code_ <= BMP_BITFIELDS in function 'readHeader'\n",
      "\n",
      "æå–å›¾åƒ:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 11216/12460 [2:27:13<25:34,  1.23s/æ–‡ä»¶, æˆåŠŸ=9174, å¤±è´¥=2042, å›¾åƒ=3855630]  [ERROR:120@8834.966] global loadsave.cpp:1324 imdecode_ imdecode_(''): can't read header: OpenCV(4.12.0) /io/opencv/modules/imgcodecs/src/grfmt_bmp.cpp:108: error: (-215:Assertion failed) m_rle_code_ >= 0 && m_rle_code_ <= BMP_BITFIELDS in function 'readHeader'\n",
      "\n",
      "æå–å›¾åƒ:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 11242/12460 [2:27:29<08:30,  2.39æ–‡ä»¶/s, æˆåŠŸ=9185, å¤±è´¥=2058, å›¾åƒ=3858921][ERROR:199@8850.511] global loadsave.cpp:1324 imdecode_ imdecode_(''): can't read header: OpenCV(4.12.0) /io/opencv/modules/imgcodecs/src/grfmt_bmp.cpp:108: error: (-215:Assertion failed) m_rle_code_ >= 0 && m_rle_code_ <= BMP_BITFIELDS in function 'readHeader'\n",
      "\n",
      "æå–å›¾åƒ: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12460/12460 [2:40:20<00:00,  1.30æ–‡ä»¶/s, æˆåŠŸ=10285, å¤±è´¥=2175, å›¾åƒ=4.4e+6] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "âœ… æå–å®Œæˆï¼\n",
      "================================================================================\n",
      "ğŸ“Š ç»Ÿè®¡:\n",
      "   æ€»æ–‡ä»¶: 12460\n",
      "   æˆåŠŸ: 10285\n",
      "   å¤±è´¥: 2175\n",
      "   å›¾åƒæ•°: 4398978\n",
      "\n",
      "ğŸ“· æŒ‰ç›¸æœºç»Ÿè®¡:\n",
      "   camera_left: 1466326 å¼ \n",
      "   camera_right: 1466326 å¼ \n",
      "   camera_top: 1466326 å¼ \n",
      "\n",
      "ğŸ’¾ è¾“å‡º: /home/vcj9002/jianshu/chengxuan/Data/robomind/data/3rgb\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "RGBå›¾åƒæ‰¹é‡æå–å·¥å…· - å¤šçº¿ç¨‹ç‰ˆæœ¬\n",
    "æ”¯æŒé«˜å¹¶å‘ï¼ˆæ¨è128çº¿ç¨‹ï¼‰\n",
    "\"\"\"\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "class RGBImageExtractor:\n",
    "    \"\"\"ä»HDF5æ‰¹é‡æå–RGBå›¾åƒï¼ˆå¤šçº¿ç¨‹ï¼‰\"\"\"\n",
    "    \n",
    "    def __init__(self, base_data_path: str, output_base_path: str):\n",
    "        self.base_data_path = Path(base_data_path)\n",
    "        self.output_base_path = Path(output_base_path)\n",
    "        self.stats = {\n",
    "            'total_h5_files': 0,\n",
    "            'successful_h5_files': 0,\n",
    "            'failed_h5_files': 0,\n",
    "            'total_images': 0,\n",
    "            'cameras': {}\n",
    "        }\n",
    "        self.errors = []\n",
    "        self.stats_lock = threading.Lock()\n",
    "        self.errors_lock = threading.Lock()\n",
    "        \n",
    "    def decode_image(self, compressed_image: np.ndarray) -> Optional[np.ndarray]:\n",
    "        \"\"\"è§£ç å‹ç¼©å›¾åƒ\"\"\"\n",
    "        if compressed_image is None or len(compressed_image) == 0:\n",
    "            return None\n",
    "        try:\n",
    "            rgb = cv2.imdecode(compressed_image, cv2.IMREAD_COLOR)\n",
    "            if rgb is None:\n",
    "                return None\n",
    "            return cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def extract_images_from_h5(self, h5_path: Path) -> Dict[str, List[np.ndarray]]:\n",
    "        \"\"\"ä»HDF5æå–æ‰€æœ‰ç›¸æœºå›¾åƒ\"\"\"\n",
    "        images_by_camera = {}\n",
    "        try:\n",
    "            with h5py.File(h5_path, 'r') as f:\n",
    "                is_compress = f.attrs.get('compress', True)\n",
    "                if 'observations' not in f or 'rgb_images' not in f['observations']:\n",
    "                    return images_by_camera\n",
    "                \n",
    "                rgb_images_group = f['observations']['rgb_images']\n",
    "                for camera_name in rgb_images_group.keys():\n",
    "                    camera_data = rgb_images_group[camera_name]\n",
    "                    images = []\n",
    "                    for frame_idx in range(len(camera_data)):\n",
    "                        try:\n",
    "                            if is_compress:\n",
    "                                img = self.decode_image(camera_data[frame_idx])\n",
    "                                if img is not None:\n",
    "                                    images.append(img)\n",
    "                            else:\n",
    "                                images.append(camera_data[frame_idx])\n",
    "                        except:\n",
    "                            continue\n",
    "                    if images:\n",
    "                        images_by_camera[camera_name] = images\n",
    "        except Exception as e:\n",
    "            self.errors.append(f\"è¯»å–å¤±è´¥ {h5_path}: {str(e)}\")\n",
    "        return images_by_camera\n",
    "    \n",
    "    def save_images_by_camera(self, images_by_camera: Dict[str, List[np.ndarray]], relative_path: Path):\n",
    "        \"\"\"æŒ‰ç›¸æœºä¿å­˜å›¾åƒï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰\"\"\"\n",
    "        local_stats = {'total_images': 0, 'cameras': {}}\n",
    "        \n",
    "        for camera_name, images in images_by_camera.items():\n",
    "            camera_output_dir = self.output_base_path / camera_name / relative_path\n",
    "            camera_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            for idx, img in enumerate(images):\n",
    "                frame_name = f\"frame_{idx:04d}.jpg\"\n",
    "                output_path = camera_output_dir / frame_name\n",
    "                img_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "                cv2.imwrite(str(output_path), img_bgr)\n",
    "                \n",
    "                local_stats['total_images'] += 1\n",
    "                if camera_name not in local_stats['cameras']:\n",
    "                    local_stats['cameras'][camera_name] = 0\n",
    "                local_stats['cameras'][camera_name] += 1\n",
    "        \n",
    "        with self.stats_lock:\n",
    "            self.stats['total_images'] += local_stats['total_images']\n",
    "            for camera, count in local_stats['cameras'].items():\n",
    "                if camera not in self.stats['cameras']:\n",
    "                    self.stats['cameras'][camera] = 0\n",
    "                self.stats['cameras'][camera] += count\n",
    "    \n",
    "    def find_all_h5_files(self) -> List[tuple]:\n",
    "        \"\"\"æŸ¥æ‰¾æ‰€æœ‰HDF5æ–‡ä»¶\"\"\"\n",
    "        h5_files = []\n",
    "        print(f\"ğŸ” æ‰«æç›®å½•: {self.base_data_path}\")\n",
    "        \n",
    "        for embodiment_dir in self.base_data_path.iterdir():\n",
    "            if not embodiment_dir.is_dir():\n",
    "                continue\n",
    "            for task_dir in embodiment_dir.iterdir():\n",
    "                if not task_dir.is_dir():\n",
    "                    continue\n",
    "                train_dir = task_dir / 'success_episodes' / 'train'\n",
    "                if not train_dir.exists():\n",
    "                    continue\n",
    "                for episode_dir in train_dir.iterdir():\n",
    "                    if not episode_dir.is_dir():\n",
    "                        continue\n",
    "                    data_dir = episode_dir / 'data'\n",
    "                    if not data_dir.exists():\n",
    "                        continue\n",
    "                    for h5_file in data_dir.glob('*.hdf5'):\n",
    "                        relative_path = Path(embodiment_dir.name) / task_dir.name / episode_dir.name\n",
    "                        h5_files.append((h5_file, relative_path))\n",
    "        return h5_files\n",
    "    \n",
    "    def process_single_h5(self, h5_path: Path, relative_path: Path) -> Dict:\n",
    "        \"\"\"å¤„ç†å•ä¸ªHDF5ï¼ˆç”¨äºå¤šçº¿ç¨‹ï¼‰\"\"\"\n",
    "        result = {'success': False, 'error': None}\n",
    "        try:\n",
    "            images_by_camera = self.extract_images_from_h5(h5_path)\n",
    "            if images_by_camera:\n",
    "                self.save_images_by_camera(images_by_camera, relative_path)\n",
    "                result['success'] = True\n",
    "            else:\n",
    "                result['error'] = f\"æ— å›¾åƒ: {relative_path}\"\n",
    "        except Exception as e:\n",
    "            result['error'] = f\"{relative_path}: {str(e)}\"\n",
    "        return result\n",
    "    \n",
    "    def extract_all(self, limit: Optional[int] = None, num_threads: int = 128):\n",
    "        \"\"\"å¤šçº¿ç¨‹æå–æ‰€æœ‰å›¾åƒ\"\"\"\n",
    "        h5_files = self.find_all_h5_files()\n",
    "        \n",
    "        if not h5_files:\n",
    "            print(\"âŒ æœªæ‰¾åˆ°ä»»ä½•HDF5æ–‡ä»¶\")\n",
    "            return\n",
    "        \n",
    "        print(f\"ğŸ“Š æ‰¾åˆ° {len(h5_files)} ä¸ªHDF5æ–‡ä»¶\")\n",
    "        print(f\"ğŸ”§ ä½¿ç”¨ {num_threads} ä¸ªçº¿ç¨‹å¹¶è¡Œå¤„ç†\\n\")\n",
    "        \n",
    "        if limit:\n",
    "            h5_files = h5_files[:limit]\n",
    "            print(f\"âš ï¸  é™åˆ¶å¤„ç†å‰ {limit} ä¸ªæ–‡ä»¶\\n\")\n",
    "        \n",
    "        self.stats['total_h5_files'] = len(h5_files)\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "            future_to_file = {\n",
    "                executor.submit(self.process_single_h5, h5_path, relative_path): (h5_path, relative_path)\n",
    "                for h5_path, relative_path in h5_files\n",
    "            }\n",
    "            \n",
    "            with tqdm(total=len(h5_files), desc=\"æå–å›¾åƒ\", unit=\"æ–‡ä»¶\") as pbar:\n",
    "                for future in as_completed(future_to_file):\n",
    "                    result = future.result()\n",
    "                    \n",
    "                    with self.stats_lock:\n",
    "                        if result['success']:\n",
    "                            self.stats['successful_h5_files'] += 1\n",
    "                        else:\n",
    "                            self.stats['failed_h5_files'] += 1\n",
    "                        pbar.set_postfix({\n",
    "                            'æˆåŠŸ': self.stats['successful_h5_files'],\n",
    "                            'å¤±è´¥': self.stats['failed_h5_files'],\n",
    "                            'å›¾åƒ': self.stats['total_images']\n",
    "                        })\n",
    "                    \n",
    "                    if result['error']:\n",
    "                        with self.errors_lock:\n",
    "                            self.errors.append(result['error'])\n",
    "                    pbar.update(1)\n",
    "        \n",
    "        self.print_summary()\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"è¾“å‡ºç»Ÿè®¡ä¿¡æ¯\"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"âœ… æå–å®Œæˆï¼\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"ğŸ“Š ç»Ÿè®¡:\")\n",
    "        print(f\"   æ€»æ–‡ä»¶: {self.stats['total_h5_files']}\")\n",
    "        print(f\"   æˆåŠŸ: {self.stats['successful_h5_files']}\")\n",
    "        print(f\"   å¤±è´¥: {self.stats['failed_h5_files']}\")\n",
    "        print(f\"   å›¾åƒæ•°: {self.stats['total_images']}\")\n",
    "        \n",
    "        if self.stats['cameras']:\n",
    "            print(f\"\\nğŸ“· æŒ‰ç›¸æœºç»Ÿè®¡:\")\n",
    "            for camera, count in sorted(self.stats['cameras'].items()):\n",
    "                print(f\"   {camera}: {count} å¼ \")\n",
    "        \n",
    "        print(f\"\\nğŸ’¾ è¾“å‡º: {self.output_base_path}\")\n",
    "        \n",
    "        if self.errors and len(self.errors) <= 20:\n",
    "            print(f\"\\nâš ï¸  é”™è¯¯ ({len(self.errors)}):\")\n",
    "            for error in self.errors[:20]:\n",
    "                print(f\"   - {error}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='RGBå›¾åƒæ‰¹é‡æå–ï¼ˆå¤šçº¿ç¨‹ï¼‰')\n",
    "    parser.add_argument('--base_path', type=str, \n",
    "                       default='/home/vcj9002/jianshu/chengxuan/Data/robomind/data/benchmark1_0_compressed',\n",
    "                       help='è¾“å…¥è·¯å¾„')\n",
    "    parser.add_argument('--output_path', type=str,\n",
    "                       default='/home/vcj9002/jianshu/chengxuan/Data/robomind/data/3rgb',\n",
    "                       help='è¾“å‡ºè·¯å¾„')\n",
    "    parser.add_argument('--limit', type=int, default=None, help='é™åˆ¶æ–‡ä»¶æ•°ï¼ˆæµ‹è¯•ç”¨ï¼‰')\n",
    "    parser.add_argument('--threads', type=int, default=128, help='çº¿ç¨‹æ•°ï¼ˆé»˜è®¤128ï¼‰')\n",
    "    \n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    extractor = RGBImageExtractor(\n",
    "        base_data_path=args.base_path,\n",
    "        output_base_path=args.output_path\n",
    "    )\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"ğŸš€ RGBå›¾åƒæ‰¹é‡æå–ï¼ˆå¤šçº¿ç¨‹ç‰ˆï¼‰\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ğŸ“‚ è¾“å…¥: {args.base_path}\")\n",
    "    print(f\"ğŸ’¾ è¾“å‡º: {args.output_path}\")\n",
    "    print(f\"ğŸ§µ çº¿ç¨‹: {args.threads}\")\n",
    "    if args.limit:\n",
    "        print(f\"âš ï¸  æµ‹è¯•: å‰ {args.limit} ä¸ªæ–‡ä»¶\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    extractor.extract_all(limit=args.limit, num_threads=args.threads)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
