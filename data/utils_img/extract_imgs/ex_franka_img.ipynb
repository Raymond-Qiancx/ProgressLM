{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fc3afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“– è¯»å–JSONLæ–‡ä»¶: /home/vcj9002/jianshu/workspace/code/ProgressLM/data/raw/converted/h5_franka_3rgb_converted.jsonl\n",
      "ğŸ“Š å‘ç° 3287 ä¸ªå”¯ä¸€IDéœ€è¦å¤„ç†\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "æå–å›¾åƒ:   2%|â–         | 81/3287 [02:02<1:20:36,  1.51s/ID, æˆåŠŸ=0, å¤±è´¥=81]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 412\u001b[39m\n\u001b[32m    404\u001b[39m     \u001b[38;5;66;03m# å¦‚æœéœ€è¦è¯Šæ–­ç‰¹å®šIDçš„é—®é¢˜ï¼Œå¯ä»¥ä½¿ç”¨ï¼š\u001b[39;00m\n\u001b[32m    405\u001b[39m     \u001b[38;5;66;03m# extractor.diagnose_path(\"h5_franka_3rgb/blue_cub_on_pink/1015_172846\")\u001b[39;00m\n\u001b[32m    406\u001b[39m \n\u001b[32m    407\u001b[39m     \u001b[38;5;66;03m# åˆ—å‡ºå¯ç”¨ä»»åŠ¡ï¼š\u001b[39;00m\n\u001b[32m    408\u001b[39m     \u001b[38;5;66;03m# extractor.list_available_tasks(\"h5_franka_3rgb\")\u001b[39;00m\n\u001b[32m    411\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m412\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[38;5;66;03m# h5_ur_1rgb/triangle_bread_on_table/1015_112426 missed\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 402\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    400\u001b[39m \u001b[38;5;66;03m# å¤„ç†JSONLæ–‡ä»¶\u001b[39;00m\n\u001b[32m    401\u001b[39m jsonl_file = \u001b[33m\"\u001b[39m\u001b[33m/home/vcj9002/jianshu/workspace/code/ProgressLM/data/raw/converted/h5_franka_3rgb_converted.jsonl\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m \u001b[43mextractor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_jsonl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjsonl_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 237\u001b[39m, in \u001b[36mImageExtractor.process_jsonl\u001b[39m\u001b[34m(self, jsonl_path, limit)\u001b[39m\n\u001b[32m    234\u001b[39m frames_to_extract = \u001b[38;5;28mlist\u001b[39m(task_info[\u001b[33m'\u001b[39m\u001b[33mframes\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m    236\u001b[39m \u001b[38;5;66;03m# ä»HDF5æå–å›¾åƒ\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m images = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mextract_specific_frames\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh5_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_to_extract\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m images:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# æ„å»ºè¾“å‡ºè·¯å¾„\u001b[39;00m\n\u001b[32m    241\u001b[39m     output_dir = \u001b[38;5;28mself\u001b[39m.output_base_path / id_info[\u001b[33m'\u001b[39m\u001b[33membodiment\u001b[39m\u001b[33m'\u001b[39m] / \\\n\u001b[32m    242\u001b[39m                 id_info[\u001b[33m'\u001b[39m\u001b[33mtask_name\u001b[39m\u001b[33m'\u001b[39m] / id_info[\u001b[33m'\u001b[39m\u001b[33mepisode_id\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 141\u001b[39m, in \u001b[36mImageExtractor.extract_specific_frames\u001b[39m\u001b[34m(self, h5_path, frame_names, camera_name)\u001b[39m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m frame_idx < \u001b[38;5;28mlen\u001b[39m(rgb_data):\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_compress:\n\u001b[32m    140\u001b[39m         \u001b[38;5;66;03m# è§£ç å‹ç¼©å›¾åƒ\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m         compressed_img = \u001b[43mrgb_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mframe_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    142\u001b[39m         img = \u001b[38;5;28mself\u001b[39m.decode_image(compressed_img)\n\u001b[32m    143\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m img \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    144\u001b[39m             \u001b[38;5;66;03m# è½¬æ¢BGRåˆ°RGB\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:56\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:57\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/h5py/_hl/dataset.py:903\u001b[39m, in \u001b[36mDataset.__getitem__\u001b[39m\u001b[34m(self, args, new_dtype)\u001b[39m\n\u001b[32m    901\u001b[39m mspace = h5s.create_simple(selection.mshape)\n\u001b[32m    902\u001b[39m fspace = selection.id\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdxpl\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dxpl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;66;03m# Patch up the output for NumPy\u001b[39;00m\n\u001b[32m    906\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m arr.shape == ():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:56\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:57\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/h5d.pyx:262\u001b[39m, in \u001b[36mh5py.h5d.DatasetID.read\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_proxy.pyx:151\u001b[39m, in \u001b[36mh5py._proxy.dset_rw\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_conv.pyx:644\u001b[39m, in \u001b[36mh5py._conv.vlen2ndarray\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/h5t.pyx:456\u001b[39m, in \u001b[36mh5py.h5t.TypeID.dtype.__get__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/h5t.pyx:457\u001b[39m, in \u001b[36mh5py.h5t.TypeID.dtype.__get__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/h5t.pyx:974\u001b[39m, in \u001b[36mh5py.h5t.TypeIntegerID.py_dtype\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import h5py\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Set\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ImageExtractor:\n",
    "    \"\"\"ä»JSONLæ–‡ä»¶è¯»å–ä¿¡æ¯å¹¶ä»HDF5æ–‡ä»¶ä¸­æå–å¯¹åº”å›¾åƒ\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 base_data_path: str = \"/home/runsheng/personal_3/qiancx/Sources/datasets/robomind/benchmark1_0_compressed\",\n",
    "                 output_base_path: str = \"/home/runsheng/personal_3/qiancx/Sources/datasets/robomind/images\"):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–æå–å™¨\n",
    "        \n",
    "        Args:\n",
    "            base_data_path: HDF5æ•°æ®çš„åŸºç¡€è·¯å¾„\n",
    "            output_base_path: è¾“å‡ºå›¾åƒçš„åŸºç¡€è·¯å¾„\n",
    "        \"\"\"\n",
    "        self.base_data_path = Path(base_data_path)\n",
    "        self.output_base_path = Path(output_base_path)\n",
    "        self.errors = []  # è®°å½•é”™è¯¯\n",
    "        \n",
    "    def parse_id(self, id_string: str) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        è§£æIDå­—ç¬¦ä¸²\n",
    "        \n",
    "        Args:\n",
    "            id_string: æ ¼å¼å¦‚ \"h5_franka_3rgb/place_in_block_in_plate_1/1014_163457\"\n",
    "            \n",
    "        Returns:\n",
    "            åŒ…å«è§£æç»“æœçš„å­—å…¸\n",
    "        \"\"\"\n",
    "        parts = id_string.split('/')\n",
    "        if len(parts) < 3:\n",
    "            raise ValueError(f\"æ— æ•ˆçš„IDæ ¼å¼: {id_string}\")\n",
    "            \n",
    "        return {\n",
    "            'embodiment': parts[0],  # h5_franka_3rgb\n",
    "            'task_name': parts[1],    # place_in_block_in_plate_1\n",
    "            'episode_id': parts[2]     # 1014_163457\n",
    "        }\n",
    "    \n",
    "    def construct_h5_path(self, id_info: Dict[str, str]) -> Path:\n",
    "        \"\"\"\n",
    "        æ ¹æ®IDä¿¡æ¯æ„å»ºHDF5æ–‡ä»¶è·¯å¾„\n",
    "        \n",
    "        Args:\n",
    "            id_info: åŒ…å«embodiment, task_name, episode_idçš„å­—å…¸\n",
    "            \n",
    "        Returns:\n",
    "            HDF5æ–‡ä»¶çš„å®Œæ•´è·¯å¾„\n",
    "        \"\"\"\n",
    "        h5_dir = self.base_data_path / id_info['embodiment'] / id_info['task_name'] / \\\n",
    "                 'success_episodes' / 'train' / id_info['episode_id'] / 'data'\n",
    "        \n",
    "        # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨\n",
    "        if not h5_dir.exists():\n",
    "            raise FileNotFoundError(f\"ç›®å½•ä¸å­˜åœ¨: {h5_dir}\")\n",
    "        \n",
    "        # æŸ¥æ‰¾HDF5æ–‡ä»¶ï¼ˆé€šå¸¸åä¸ºtrajectory.hdf5ï¼‰\n",
    "        h5_files = list(h5_dir.glob('*.hdf5'))\n",
    "        if not h5_files:\n",
    "            raise FileNotFoundError(f\"æœªæ‰¾åˆ°HDF5æ–‡ä»¶åœ¨: {h5_dir}\")\n",
    "        \n",
    "        # ä¼˜å…ˆé€‰æ‹©trajectory.hdf5\n",
    "        for h5_file in h5_files:\n",
    "            if h5_file.name == 'trajectory.hdf5':\n",
    "                return h5_file\n",
    "        \n",
    "        return h5_files[0]  # å¦‚æœæ²¡æœ‰trajectory.hdf5ï¼Œè¿”å›ç¬¬ä¸€ä¸ªHDF5æ–‡ä»¶\n",
    "    \n",
    "    def decode_image(self, compressed_image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        è§£ç å‹ç¼©çš„å›¾åƒæ•°æ®\n",
    "        \n",
    "        Args:\n",
    "            compressed_image: å‹ç¼©çš„å›¾åƒæ•°æ®\n",
    "            \n",
    "        Returns:\n",
    "            è§£ç åçš„RGBå›¾åƒ\n",
    "        \"\"\"\n",
    "        if compressed_image is None:\n",
    "            return None\n",
    "            \n",
    "        # å¦‚æœæ˜¯å•ä¸ªå›¾åƒ\n",
    "        if len(compressed_image.shape) == 1:\n",
    "            rgb = cv2.imdecode(compressed_image, cv2.IMREAD_COLOR)\n",
    "            if rgb is None:\n",
    "                # å°è¯•ç›´æ¥è§£æä¸ºæ•°ç»„\n",
    "                rgb = np.frombuffer(compressed_image, dtype=np.uint8)\n",
    "                # æ ¹æ®å¤§å°åˆ¤æ–­å›¾åƒå°ºå¯¸\n",
    "                if rgb.size == 2764800:\n",
    "                    rgb = rgb.reshape(720, 1280, 3)\n",
    "                elif rgb.size == 921600:\n",
    "                    rgb = rgb.reshape(480, 640, 3)\n",
    "            return rgb\n",
    "        else:\n",
    "            # å¦‚æœæ˜¯æ‰¹é‡å›¾åƒï¼Œè¿”å›Noneï¼ˆéœ€è¦ç‰¹æ®Šå¤„ç†ï¼‰\n",
    "            return None\n",
    "    \n",
    "    def extract_specific_frames(self, h5_path: Path, frame_names: List[str], \n",
    "                               camera_name: str = 'camera_top') -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        ä»HDF5æ–‡ä»¶ä¸­æå–ç‰¹å®šå¸§\n",
    "        \n",
    "        Args:\n",
    "            h5_path: HDF5æ–‡ä»¶è·¯å¾„\n",
    "            frame_names: è¦æå–çš„å¸§åç§°åˆ—è¡¨ï¼ˆå¦‚ [\"camera_top_0000.jpg\", \"camera_top_0281.jpg\"]ï¼‰\n",
    "            camera_name: ç›¸æœºåç§°\n",
    "            \n",
    "        Returns:\n",
    "            å¸§åç§°åˆ°å›¾åƒæ•°ç»„çš„æ˜ å°„\n",
    "        \"\"\"\n",
    "        images = {}\n",
    "        \n",
    "        with h5py.File(h5_path, 'r') as f:\n",
    "            # æ£€æŸ¥æ˜¯å¦å‹ç¼©\n",
    "            is_compress = f.attrs.get('compress', True)\n",
    "            \n",
    "            # è·å–RGBå›¾åƒæ•°æ®\n",
    "            if 'observations' in f and 'rgb_images' in f['observations']:\n",
    "                if camera_name in f['observations']['rgb_images']:\n",
    "                    rgb_data = f['observations']['rgb_images'][camera_name]\n",
    "                    \n",
    "                    # è§£æå¸§ç´¢å¼•\n",
    "                    for frame_name in frame_names:\n",
    "                        # ä»æ–‡ä»¶åæå–å¸§ç´¢å¼•ï¼ˆå¦‚ camera_top_0000.jpg -> 0ï¼‰\n",
    "                        try:\n",
    "                            # æå–æ•°å­—éƒ¨åˆ†\n",
    "                            frame_idx_str = frame_name.split('_')[-1].split('.')[0]\n",
    "                            frame_idx = int(frame_idx_str)\n",
    "                            \n",
    "                            if frame_idx < len(rgb_data):\n",
    "                                if is_compress:\n",
    "                                    # è§£ç å‹ç¼©å›¾åƒ\n",
    "                                    compressed_img = rgb_data[frame_idx]\n",
    "                                    img = self.decode_image(compressed_img)\n",
    "                                    if img is not None:\n",
    "                                        # è½¬æ¢BGRåˆ°RGB\n",
    "                                        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                                        images[frame_name] = img\n",
    "                                else:\n",
    "                                    # ç›´æ¥è¯»å–æœªå‹ç¼©å›¾åƒ\n",
    "                                    images[frame_name] = rgb_data[frame_idx]\n",
    "                        except (ValueError, IndexError):\n",
    "                            continue\n",
    "            \n",
    "        return images\n",
    "    \n",
    "    def save_images(self, images: Dict[str, np.ndarray], output_dir: Path):\n",
    "        \"\"\"\n",
    "        ä¿å­˜å›¾åƒåˆ°æŒ‡å®šç›®å½•\n",
    "        \n",
    "        Args:\n",
    "            images: å¸§åç§°åˆ°å›¾åƒæ•°ç»„çš„æ˜ å°„\n",
    "            output_dir: è¾“å‡ºç›®å½•\n",
    "        \"\"\"\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        for frame_name, img in images.items():\n",
    "            output_path = output_dir / frame_name\n",
    "            # è½¬æ¢RGBå›BGRç”¨äºOpenCVä¿å­˜\n",
    "            img_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "            cv2.imwrite(str(output_path), img_bgr)\n",
    "    \n",
    "    def process_jsonl(self, jsonl_path: str, limit: int = None):\n",
    "        \"\"\"\n",
    "        å¤„ç†JSONLæ–‡ä»¶å¹¶æå–æ‰€æœ‰å›¾åƒ\n",
    "        \n",
    "        Args:\n",
    "            jsonl_path: JSONLæ–‡ä»¶è·¯å¾„\n",
    "            limit: é™åˆ¶å¤„ç†çš„è¡Œæ•°ï¼ˆç”¨äºæµ‹è¯•ï¼‰\n",
    "        \"\"\"\n",
    "        # é‡ç½®é”™è¯¯åˆ—è¡¨\n",
    "        self.errors = []\n",
    "        \n",
    "        # æ”¶é›†æ‰€æœ‰éœ€è¦æå–çš„æ•°æ®\n",
    "        extraction_tasks = defaultdict(lambda: defaultdict(set))\n",
    "        \n",
    "        print(f\"ğŸ“– è¯»å–JSONLæ–‡ä»¶: {jsonl_path}\")\n",
    "        \n",
    "        # è¯»å–JSONLæ–‡ä»¶\n",
    "        with open(jsonl_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            if limit:\n",
    "                lines = lines[:limit]\n",
    "                \n",
    "            for line_num, line in enumerate(lines, 1):\n",
    "                try:\n",
    "                    data = json.loads(line.strip())\n",
    "                    id_string = data['id']\n",
    "                    \n",
    "                    # æ”¶é›†æ‰€æœ‰éœ€è¦çš„å¸§\n",
    "                    all_frames = set()\n",
    "                    \n",
    "                    # visual_demoä¸­çš„å¸§\n",
    "                    if 'visual_demo' in data:\n",
    "                        all_frames.update(data['visual_demo'])\n",
    "                    \n",
    "                    # stage_to_estimateä¸­çš„å¸§\n",
    "                    if 'stage_to_estimate' in data:\n",
    "                        all_frames.update(data['stage_to_estimate'])\n",
    "                    \n",
    "                    # æŒ‰IDå’Œç›¸æœºåˆ†ç»„\n",
    "                    extraction_tasks[id_string]['frames'].update(all_frames)\n",
    "                    \n",
    "                except json.JSONDecodeError:\n",
    "                    self.errors.append(f\"ç¬¬ {line_num} è¡Œ: JSONè§£æé”™è¯¯\")\n",
    "                except Exception as e:\n",
    "                    self.errors.append(f\"ç¬¬ {line_num} è¡Œ: {str(e)}\")\n",
    "        \n",
    "        print(f\"ğŸ“Š å‘ç° {len(extraction_tasks)} ä¸ªå”¯ä¸€IDéœ€è¦å¤„ç†\\n\")\n",
    "        \n",
    "        # ç»Ÿè®¡æˆåŠŸå’Œå¤±è´¥\n",
    "        success_count = 0\n",
    "        failed_count = 0\n",
    "        \n",
    "        # ä½¿ç”¨tqdmå¤„ç†æ¯ä¸ªå”¯ä¸€çš„ID\n",
    "        with tqdm(total=len(extraction_tasks), desc=\"æå–å›¾åƒ\", unit=\"ID\") as pbar:\n",
    "            for id_string, task_info in extraction_tasks.items():\n",
    "                try:\n",
    "                    # è§£æID\n",
    "                    id_info = self.parse_id(id_string)\n",
    "                    \n",
    "                    # æ„å»ºHDF5è·¯å¾„\n",
    "                    h5_path = self.construct_h5_path(id_info)\n",
    "                    \n",
    "                    # æå–æ‰€æœ‰å¸§\n",
    "                    frames_to_extract = list(task_info['frames'])\n",
    "                    \n",
    "                    # ä»HDF5æå–å›¾åƒ\n",
    "                    images = self.extract_specific_frames(h5_path, frames_to_extract)\n",
    "                    \n",
    "                    if images:\n",
    "                        # æ„å»ºè¾“å‡ºè·¯å¾„\n",
    "                        output_dir = self.output_base_path / id_info['embodiment'] / \\\n",
    "                                    id_info['task_name'] / id_info['episode_id']\n",
    "                        \n",
    "                        # ä¿å­˜å›¾åƒ\n",
    "                        self.save_images(images, output_dir)\n",
    "                        success_count += 1\n",
    "                        \n",
    "                        # æ›´æ–°è¿›åº¦æ¡æè¿°\n",
    "                        pbar.set_postfix({\n",
    "                            'æˆåŠŸ': success_count, \n",
    "                            'å¤±è´¥': failed_count,\n",
    "                            'å½“å‰': f\"{id_info['task_name'][:20]}...\"\n",
    "                        })\n",
    "                    else:\n",
    "                        failed_count += 1\n",
    "                        self.errors.append(f\"ID {id_string}: æœªèƒ½æå–ä»»ä½•å›¾åƒ\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    failed_count += 1\n",
    "                    self.errors.append(f\"ID {id_string}: {str(e)}\")\n",
    "                    pbar.set_postfix({'æˆåŠŸ': success_count, 'å¤±è´¥': failed_count})\n",
    "                \n",
    "                pbar.update(1)\n",
    "        \n",
    "        # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡\n",
    "        print(f\"\\nâœ… å¤„ç†å®Œæˆï¼\")\n",
    "        print(f\"   æˆåŠŸ: {success_count} ä¸ªID\")\n",
    "        print(f\"   å¤±è´¥: {failed_count} ä¸ªID\")\n",
    "        \n",
    "        # å¦‚æœæœ‰é”™è¯¯ï¼Œè¯¢é—®æ˜¯å¦æŸ¥çœ‹\n",
    "        if self.errors:\n",
    "            print(f\"\\nâš ï¸ å‘ç° {len(self.errors)} ä¸ªé”™è¯¯\")\n",
    "            response = input(\"æ˜¯å¦æŸ¥çœ‹é”™è¯¯è¯¦æƒ…ï¼Ÿ(y/n): \")\n",
    "            if response.lower() == 'y':\n",
    "                print(\"\\né”™è¯¯åˆ—è¡¨:\")\n",
    "                for i, error in enumerate(self.errors, 1):\n",
    "                    print(f\"  {i}. {error}\")\n",
    "    \n",
    "    def diagnose_path(self, id_string: str):\n",
    "        \"\"\"\n",
    "        è¯Šæ–­è·¯å¾„é—®é¢˜ï¼Œå¸®åŠ©è°ƒè¯•\n",
    "        \n",
    "        Args:\n",
    "            id_string: IDå­—ç¬¦ä¸²\n",
    "        \"\"\"\n",
    "        print(f\"\\nğŸ” è¯Šæ–­ID: {id_string}\")\n",
    "        \n",
    "        try:\n",
    "            id_info = self.parse_id(id_string)\n",
    "            print(f\"  è§£æç»“æœ:\")\n",
    "            print(f\"    - Embodiment: {id_info['embodiment']}\")\n",
    "            print(f\"    - Task: {id_info['task_name']}\")\n",
    "            print(f\"    - Episode: {id_info['episode_id']}\")\n",
    "            \n",
    "            # æ£€æŸ¥embodimentç›®å½•\n",
    "            embodiment_path = self.base_data_path / id_info['embodiment']\n",
    "            print(f\"\\n  æ£€æŸ¥embodimentç›®å½•: {embodiment_path}\")\n",
    "            if embodiment_path.exists():\n",
    "                print(f\"    âœ“ ç›®å½•å­˜åœ¨\")\n",
    "                \n",
    "                # åˆ—å‡ºæ‰€æœ‰ä»»åŠ¡\n",
    "                task_dirs = sorted([d.name for d in embodiment_path.iterdir() if d.is_dir()])\n",
    "                print(f\"    ğŸ“ æ‰¾åˆ° {len(task_dirs)} ä¸ªä»»åŠ¡ç›®å½•\")\n",
    "                \n",
    "                # æŸ¥æ‰¾ç›¸ä¼¼çš„ä»»åŠ¡å\n",
    "                from difflib import get_close_matches\n",
    "                similar = get_close_matches(id_info['task_name'], task_dirs, n=3, cutoff=0.6)\n",
    "                if similar:\n",
    "                    print(f\"    ğŸ’¡ ç›¸ä¼¼çš„ä»»åŠ¡å: {similar}\")\n",
    "                \n",
    "                # æ£€æŸ¥ç¡®åˆ‡çš„ä»»åŠ¡ç›®å½•\n",
    "                task_path = embodiment_path / id_info['task_name']\n",
    "                if task_path.exists():\n",
    "                    print(f\"\\n  âœ“ ä»»åŠ¡ç›®å½•å­˜åœ¨: {task_path}\")\n",
    "                    \n",
    "                    # æ£€æŸ¥episodeè·¯å¾„\n",
    "                    episode_path = task_path / 'success_episodes' / 'train' / id_info['episode_id']\n",
    "                    if episode_path.exists():\n",
    "                        print(f\"  âœ“ Episodeç›®å½•å­˜åœ¨: {episode_path}\")\n",
    "                        \n",
    "                        # æ£€æŸ¥dataç›®å½•\n",
    "                        data_path = episode_path / 'data'\n",
    "                        if data_path.exists():\n",
    "                            print(f\"  âœ“ Dataç›®å½•å­˜åœ¨: {data_path}\")\n",
    "                            \n",
    "                            # åˆ—å‡ºæ‰€æœ‰æ–‡ä»¶\n",
    "                            files = list(data_path.iterdir())\n",
    "                            print(f\"  ğŸ“ ç›®å½•å†…å®¹:\")\n",
    "                            for f in files:\n",
    "                                print(f\"      - {f.name} ({f.stat().st_size / 1024 / 1024:.2f} MB)\")\n",
    "                            \n",
    "                            # æŸ¥æ‰¾HDF5æ–‡ä»¶\n",
    "                            h5_files = list(data_path.glob('*.hdf5'))\n",
    "                            if h5_files:\n",
    "                                print(f\"  âœ… æ‰¾åˆ°HDF5æ–‡ä»¶: {h5_files[0].name}\")\n",
    "                            else:\n",
    "                                print(f\"  âŒ æœªæ‰¾åˆ°HDF5æ–‡ä»¶\")\n",
    "                        else:\n",
    "                            print(f\"  âŒ Dataç›®å½•ä¸å­˜åœ¨\")\n",
    "                    else:\n",
    "                        print(f\"  âŒ Episodeç›®å½•ä¸å­˜åœ¨\")\n",
    "                        # åˆ—å‡ºå¯ç”¨çš„episodes\n",
    "                        train_path = task_path / 'success_episodes' / 'train'\n",
    "                        if train_path.exists():\n",
    "                            episodes = sorted([d.name for d in train_path.iterdir() if d.is_dir()])[:5]\n",
    "                            print(f\"  ğŸ’¡ å¯ç”¨çš„episodesç¤ºä¾‹: {episodes}\")\n",
    "                else:\n",
    "                    print(f\"  âŒ ä»»åŠ¡ç›®å½•ä¸å­˜åœ¨: {id_info['task_name']}\")\n",
    "                    print(f\"  ğŸ’¡ è¯·æ£€æŸ¥ä»»åŠ¡åæ˜¯å¦æ­£ç¡®\")\n",
    "            else:\n",
    "                print(f\"    âŒ Embodimentç›®å½•ä¸å­˜åœ¨\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ è¯Šæ–­å¤±è´¥: {e}\")\n",
    "    \n",
    "    def list_available_tasks(self, embodiment: str = \"h5_franka_3rgb\"):\n",
    "        \"\"\"\n",
    "        åˆ—å‡ºæŒ‡å®šembodimentä¸‹çš„æ‰€æœ‰å¯ç”¨ä»»åŠ¡\n",
    "        \n",
    "        Args:\n",
    "            embodiment: embodimentåç§°\n",
    "        \"\"\"\n",
    "        embodiment_path = self.base_data_path / embodiment\n",
    "        \n",
    "        if not embodiment_path.exists():\n",
    "            print(f\"âŒ Embodimentç›®å½•ä¸å­˜åœ¨: {embodiment_path}\")\n",
    "            return []\n",
    "        \n",
    "        tasks = []\n",
    "        for task_dir in sorted(embodiment_path.iterdir()):\n",
    "            if task_dir.is_dir():\n",
    "                # æ£€æŸ¥æ˜¯å¦æœ‰success_episodes/trainç›®å½•\n",
    "                train_path = task_dir / 'success_episodes' / 'train'\n",
    "                if train_path.exists():\n",
    "                    episodes = list(train_path.iterdir())\n",
    "                    if episodes:\n",
    "                        tasks.append({\n",
    "                            'name': task_dir.name,\n",
    "                            'episodes': len(episodes)\n",
    "                        })\n",
    "        \n",
    "        print(f\"\\nğŸ“‹ {embodiment} ä¸‹çš„å¯ç”¨ä»»åŠ¡:\")\n",
    "        print(f\"{'ä»»åŠ¡åç§°':<50} {'Episodesæ•°é‡':>15}\")\n",
    "        print(\"-\" * 65)\n",
    "        for task in tasks:\n",
    "            print(f\"{task['name']:<50} {task['episodes']:>15}\")\n",
    "        \n",
    "        return tasks\n",
    "\n",
    "\n",
    "# ========== ä½¿ç”¨ç¤ºä¾‹ ==========\n",
    "\n",
    "def main():\n",
    "    # åˆ›å»ºæå–å™¨\n",
    "    extractor = ImageExtractor(\n",
    "        base_data_path=\"/home/vcj9002/jianshu/workspace/data/robomind/data/benchmark1_0_compressed\",\n",
    "        output_base_path=\"/home/vcj9002/jianshu/workspace/data/robomind/data/images\"\n",
    "    )\n",
    "    \n",
    "    # å¤„ç†JSONLæ–‡ä»¶\n",
    "    jsonl_file = \"/home/vcj9002/jianshu/workspace/code/ProgressLM/data/raw/converted/h5_franka_3rgb_converted.jsonl\"\n",
    "    extractor.process_jsonl(jsonl_file)\n",
    "    \n",
    "    # å¦‚æœéœ€è¦è¯Šæ–­ç‰¹å®šIDçš„é—®é¢˜ï¼Œå¯ä»¥ä½¿ç”¨ï¼š\n",
    "    # extractor.diagnose_path(\"h5_franka_3rgb/blue_cub_on_pink/1015_172846\")\n",
    "    \n",
    "    # åˆ—å‡ºå¯ç”¨ä»»åŠ¡ï¼š\n",
    "    # extractor.list_available_tasks(\"h5_franka_3rgb\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "# h5_ur_1rgb/triangle_bread_on_table/1015_112426 missed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
