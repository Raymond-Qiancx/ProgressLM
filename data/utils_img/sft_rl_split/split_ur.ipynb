{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a6836a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ æˆåŠŸåŠ è½½ 23508 æ¡æ•°æ®\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š åŠ¨ä½œç±»å‹ç»Ÿè®¡åˆ†æ\n",
      "============================================================\n",
      "\n",
      "æ€»è®¡ï¼š\n",
      "  â€¢ åŠ¨ä½œç±»å‹æ•°é‡: 74\n",
      "  â€¢ Trajectoryæ€»æ•°: 2624\n",
      "  â€¢ æ ·æœ¬æ€»æ•°: 23508\n",
      "\n",
      "è¯¦ç»†ç»Ÿè®¡ï¼š\n",
      "                                  åŠ¨ä½œç±»å‹  Trajectoryæ•°é‡  æ ·æœ¬æ•°é‡\n",
      "                close_top_white_drawer            55   612\n",
      "                       close_trash_can            55   594\n",
      "                         cover_pot_lid            51   525\n",
      "                 open_top_white_drawer            46   516\n",
      "                  red_pepper_in_basket            47   489\n",
      "                   red_pepper_on_table            47   468\n",
      "                        open_trash_can            48   456\n",
      "               yellow_pepper_in_basket            45   453\n",
      "                      close_top_drawer            40   444\n",
      "                  put_pot_lid_on_table            48   435\n",
      "                       open_top_drawer            38   429\n",
      "                          open_pot_lid            48   429\n",
      "                       pick_up_pot_lid            50   423\n",
      "                green_pepper_in_basket            36   420\n",
      "                 green_pepper_on_table            39   408\n",
      "             yellow_pepper_in_basket_1            41   405\n",
      "                yellow_pepper_on_table            40   381\n",
      "                    pick_up_long_bread            43   378\n",
      "    put_mangosteen_in_top_white_drawer            40   372\n",
      "          put_pear_in_top_white_drawer            37   366\n",
      "    put_red_pepper_in_top_white_drawer            39   363\n",
      "                   pick_up_round_bread            43   360\n",
      "   put_bread_slice_in_top_white_drawer            39   357\n",
      "                    pick_up_red_pepper            41   348\n",
      "       put_plastic_bottle_in_trash_can            39   348\n",
      "           put_paper_ball_in_trash_can            41   348\n",
      "   put_round_bread_in_top_white_drawer            40   342\n",
      "                        bread_on_table            35   336\n",
      "                   bread_in_basket_old            36   333\n",
      "                 pick_up_yellow_pepper            41   330\n",
      "                pick_up_plastic_bottle            41   330\n",
      "                   pick_up_bread_slice            42   330\n",
      "put_triangle_bread_in_top_white_drawer            37   327\n",
      " put_yellow_pepper_in_top_white_drawer            39   324\n",
      "                         pick_up_donut            39   324\n",
      "                pick_up_triangle_bread            41   324\n",
      "                put_green_onion_in_pot            37   324\n",
      "              put_banana_in_top_drawer            35   321\n",
      "  put_green_pepper_in_top_white_drawer            39   315\n",
      "         put_square_bread_in_trash_can            35   312\n",
      "                  pick_up_square_bread            41   309\n",
      "                    pick_up_paper_ball            37   303\n",
      "                          pick_up_pear            39   300\n",
      "          put_round_bread_in_trash_can            38   300\n",
      "                         pick_up_toast            38   297\n",
      "  put_square_bread_in_top_white_drawer            34   294\n",
      "                   pick_up_green_onion            35   291\n",
      "                 put_long_bread_in_pot            33   291\n",
      "                  pick_up_green_pepper            39   282\n",
      "    put_long_bread_in_top_white_drawer            37   279\n",
      "         put_donut_in_top_white_drawer            34   279\n",
      "                           pick_up_egg            36   273\n",
      "                    pick_up_mangosteen            38   273\n",
      "                        pick_up_banana            36   267\n",
      "           put_egg_in_top_white_drawer            29   261\n",
      "                  put_can_in_trash_can            30   255\n",
      "                 put_red_pepper_in_pot            27   234\n",
      "           put_long_bread_in_trash_can            23   225\n",
      "                     bread_in_basket_1            18   216\n",
      "                put_round_bread_in_pot            23   210\n",
      "                      put_toast_in_pot            26   207\n",
      "                           pick_up_can            30   204\n",
      "               put_green_pepper_in_pot            24   201\n",
      "                 pick_up_round_bread_1            24   195\n",
      "                      put_donut_in_pot            22   189\n",
      "                put_bread_slice_in_pot            21   186\n",
      "       put_yellow_pepper_in_top_drawer            20   183\n",
      "              put_yellow_pepper_in_pot            23   177\n",
      "         put_toast_in_top_white_drawer            22   174\n",
      "             put_triangle_bread_in_pot            22   171\n",
      "                      put_bread_in_pot            16   147\n",
      "                         pick_up_bread            16   144\n",
      "              green_pepper_in_basket_1            10   114\n",
      "                        put_egg_in_pot            10    78\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "class ActionAnalyzer:\n",
    "    \"\"\"åˆ†æJSONLæ ¼å¼çš„åŠ¨ä½œç±»å‹æ•°æ®\"\"\"\n",
    "    \n",
    "    def __init__(self, jsonl_path):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–åˆ†æå™¨\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        jsonl_path : str\n",
    "            JSONLæ–‡ä»¶è·¯å¾„\n",
    "        \"\"\"\n",
    "        self.jsonl_path = jsonl_path\n",
    "        self.data = []\n",
    "        self.stats = None\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"åŠ è½½JSONLæ•°æ®\"\"\"\n",
    "        with open(self.jsonl_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    self.data.append(json.loads(line))\n",
    "        print(f\"âœ“ æˆåŠŸåŠ è½½ {len(self.data)} æ¡æ•°æ®\")\n",
    "        return self\n",
    "    \n",
    "    def extract_action_type(self, id_string):\n",
    "        \"\"\"\n",
    "        ä»IDä¸­æå–åŠ¨ä½œç±»å‹\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        id_string : str\n",
    "            æ ¼å¼: æ•°æ®é›†åç§°/åŠ¨ä½œç±»å‹/æ—¶é—´æˆ³\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        str : åŠ¨ä½œç±»å‹\n",
    "        \"\"\"\n",
    "        parts = id_string.split('/')\n",
    "        if len(parts) >= 2:\n",
    "            return parts[1]  # è¿”å›ä¸¤ä¸ªæ–œæ ä¹‹é—´çš„å†…å®¹\n",
    "        return None\n",
    "    \n",
    "    def analyze(self):\n",
    "        \"\"\"åˆ†ææ•°æ®å¹¶ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯\"\"\"\n",
    "        action_data = defaultdict(lambda: {'trajectories': set(), 'samples': 0})\n",
    "        \n",
    "        for item in self.data:\n",
    "            id_string = item.get('id', '')\n",
    "            action_type = self.extract_action_type(id_string)\n",
    "            \n",
    "            if action_type:\n",
    "                # ç»Ÿè®¡trajectoryï¼ˆå®Œæ•´çš„IDä½œä¸ºå”¯ä¸€æ ‡è¯†ï¼‰\n",
    "                action_data[action_type]['trajectories'].add(id_string)\n",
    "                # ç»Ÿè®¡æ ·æœ¬æ•°\n",
    "                action_data[action_type]['samples'] += 1\n",
    "        \n",
    "        # è½¬æ¢ä¸ºDataFrame\n",
    "        stats_list = []\n",
    "        for action_type, info in action_data.items():\n",
    "            stats_list.append({\n",
    "                'åŠ¨ä½œç±»å‹': action_type,\n",
    "                'Trajectoryæ•°é‡': len(info['trajectories']),\n",
    "                'æ ·æœ¬æ•°é‡': info['samples']\n",
    "            })\n",
    "        \n",
    "        self.stats = pd.DataFrame(stats_list).sort_values('æ ·æœ¬æ•°é‡', ascending=False)\n",
    "        return self\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"æ‰“å°ç»Ÿè®¡æ‘˜è¦\"\"\"\n",
    "        if self.stats is None:\n",
    "            print(\"è¯·å…ˆè¿è¡Œ analyze() æ–¹æ³•\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ“Š åŠ¨ä½œç±»å‹ç»Ÿè®¡åˆ†æ\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"\\næ€»è®¡ï¼š\")\n",
    "        print(f\"  â€¢ åŠ¨ä½œç±»å‹æ•°é‡: {len(self.stats)}\")\n",
    "        print(f\"  â€¢ Trajectoryæ€»æ•°: {self.stats['Trajectoryæ•°é‡'].sum()}\")\n",
    "        print(f\"  â€¢ æ ·æœ¬æ€»æ•°: {self.stats['æ ·æœ¬æ•°é‡'].sum()}\")\n",
    "        \n",
    "        print(f\"\\nè¯¦ç»†ç»Ÿè®¡ï¼š\")\n",
    "        print(self.stats.to_string(index=False))\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        \n",
    "    def plot_statistics(self, figsize=(14, 6)):\n",
    "        \"\"\"ç»˜åˆ¶ç»Ÿè®¡å›¾è¡¨\"\"\"\n",
    "        if self.stats is None:\n",
    "            print(\"è¯·å…ˆè¿è¡Œ analyze() æ–¹æ³•\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
    "        \n",
    "        # å›¾1ï¼šTrajectoryæ•°é‡\n",
    "        axes[0].barh(self.stats['åŠ¨ä½œç±»å‹'], self.stats['Trajectoryæ•°é‡'], \n",
    "                     color='steelblue', alpha=0.8)\n",
    "        axes[0].set_xlabel('Trajectoryæ•°é‡', fontsize=11)\n",
    "        axes[0].set_title('å„åŠ¨ä½œç±»å‹çš„Trajectoryæ•°é‡', fontsize=12, fontweight='bold')\n",
    "        axes[0].grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        # å›¾2ï¼šæ ·æœ¬æ•°é‡\n",
    "        axes[1].barh(self.stats['åŠ¨ä½œç±»å‹'], self.stats['æ ·æœ¬æ•°é‡'], \n",
    "                     color='coral', alpha=0.8)\n",
    "        axes[1].set_xlabel('æ ·æœ¬æ•°é‡', fontsize=11)\n",
    "        axes[1].set_title('å„åŠ¨ä½œç±»å‹çš„æ ·æœ¬æ•°é‡', fontsize=12, fontweight='bold')\n",
    "        axes[1].grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def get_dataframe(self):\n",
    "        \"\"\"è¿”å›ç»Ÿè®¡DataFrame\"\"\"\n",
    "        return self.stats\n",
    "    \n",
    "    def export_to_csv(self, output_path='action_statistics.csv'):\n",
    "        \"\"\"å¯¼å‡ºç»Ÿè®¡ç»“æœåˆ°CSV\"\"\"\n",
    "        if self.stats is None:\n",
    "            print(\"è¯·å…ˆè¿è¡Œ analyze() æ–¹æ³•\")\n",
    "            return\n",
    "        \n",
    "        self.stats.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"âœ“ ç»Ÿè®¡ç»“æœå·²ä¿å­˜åˆ°: {output_path}\")\n",
    "\n",
    "\n",
    "# ==================== ä½¿ç”¨ç¤ºä¾‹ ====================\n",
    "\n",
    "# 1. åˆ›å»ºåˆ†æå™¨å¹¶åŠ è½½æ•°æ®\n",
    "analyzer = ActionAnalyzer('/home/runsheng/personal_3/qiancx/ProgressLM/data/h5_ur_1rgb_converted.jsonl')  # æ›¿æ¢ä¸ºä½ çš„æ–‡ä»¶è·¯å¾„\n",
    "analyzer.load_data()\n",
    "\n",
    "# 2. æ‰§è¡Œåˆ†æ\n",
    "analyzer.analyze()\n",
    "\n",
    "# 3. æ‰“å°ç»Ÿè®¡æ‘˜è¦\n",
    "analyzer.print_summary()\n",
    "\n",
    "# # 4. ç»˜åˆ¶å¯è§†åŒ–å›¾è¡¨\n",
    "# analyzer.plot_statistics()\n",
    "\n",
    "# # 5. è·å–DataFrameè¿›è¡Œè¿›ä¸€æ­¥åˆ†æ\n",
    "# df = analyzer.get_dataframe()\n",
    "# display(df)  # åœ¨Jupyterä¸­æ˜¾ç¤ºè¡¨æ ¼\n",
    "\n",
    "# 6. å¯¼å‡ºç»“æœï¼ˆå¯é€‰ï¼‰\n",
    "# analyzer.export_to_csv('action_statistics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0474d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ æˆåŠŸåŠ è½½ 23508 æ¡æ•°æ®\n",
      "âœ“ å·²å†™å‡º Train: 18531 æ¡ -> ./home/runsheng/personal_3/qiancx/Sources/datasets/robomind/progrsslm/annotation/h5_ur_1rgb/h5_ur_1rgb_train.jsonl\n",
      "âœ“ å·²å†™å‡º Test : 4977 æ¡ -> ./home/runsheng/personal_3/qiancx/Sources/datasets/robomind/progrsslm/annotation/h5_ur_1rgb/h5_ur_1rgb_test.jsonl\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š Train ç»Ÿè®¡\n",
      "============================================================\n",
      "  â€¢ åŠ¨ä½œç±»å‹æ•°é‡: 64\n",
      "  â€¢ Trajectoryæ€»æ•°: 2134\n",
      "  â€¢ æ ·æœ¬æ€»æ•°: 18531\n",
      "\n",
      "è¯¦ç»†ç»Ÿè®¡ï¼š\n",
      "                                  åŠ¨ä½œç±»å‹  Trajectoryæ•°é‡  æ ·æœ¬æ•°é‡\n",
      "                      close_top_drawer            40   444\n",
      "                       open_top_drawer            38   429\n",
      "                       pick_up_pot_lid            50   423\n",
      "                green_pepper_in_basket            36   420\n",
      "                 green_pepper_on_table            39   408\n",
      "             yellow_pepper_in_basket_1            41   405\n",
      "                yellow_pepper_on_table            40   381\n",
      "                    pick_up_long_bread            43   378\n",
      "    put_mangosteen_in_top_white_drawer            40   372\n",
      "          put_pear_in_top_white_drawer            37   366\n",
      "    put_red_pepper_in_top_white_drawer            39   363\n",
      "                   pick_up_round_bread            43   360\n",
      "   put_bread_slice_in_top_white_drawer            39   357\n",
      "           put_paper_ball_in_trash_can            41   348\n",
      "                    pick_up_red_pepper            41   348\n",
      "       put_plastic_bottle_in_trash_can            39   348\n",
      "   put_round_bread_in_top_white_drawer            40   342\n",
      "                        bread_on_table            35   336\n",
      "                   bread_in_basket_old            36   333\n",
      "                pick_up_plastic_bottle            41   330\n",
      "                 pick_up_yellow_pepper            41   330\n",
      "                   pick_up_bread_slice            42   330\n",
      "put_triangle_bread_in_top_white_drawer            37   327\n",
      "                pick_up_triangle_bread            41   324\n",
      " put_yellow_pepper_in_top_white_drawer            39   324\n",
      "                put_green_onion_in_pot            37   324\n",
      "                         pick_up_donut            39   324\n",
      "              put_banana_in_top_drawer            35   321\n",
      "  put_green_pepper_in_top_white_drawer            39   315\n",
      "         put_square_bread_in_trash_can            35   312\n",
      "                  pick_up_square_bread            41   309\n",
      "                    pick_up_paper_ball            37   303\n",
      "                          pick_up_pear            39   300\n",
      "          put_round_bread_in_trash_can            38   300\n",
      "                         pick_up_toast            38   297\n",
      "  put_square_bread_in_top_white_drawer            34   294\n",
      "                   pick_up_green_onion            35   291\n",
      "                 put_long_bread_in_pot            33   291\n",
      "                  pick_up_green_pepper            39   282\n",
      "    put_long_bread_in_top_white_drawer            37   279\n",
      "         put_donut_in_top_white_drawer            34   279\n",
      "                           pick_up_egg            36   273\n",
      "                    pick_up_mangosteen            38   273\n",
      "                        pick_up_banana            36   267\n",
      "           put_egg_in_top_white_drawer            29   261\n",
      "                  put_can_in_trash_can            30   255\n",
      "                 put_red_pepper_in_pot            27   234\n",
      "           put_long_bread_in_trash_can            23   225\n",
      "                     bread_in_basket_1            18   216\n",
      "                put_round_bread_in_pot            23   210\n",
      "                      put_toast_in_pot            26   207\n",
      "                           pick_up_can            30   204\n",
      "               put_green_pepper_in_pot            24   201\n",
      "                 pick_up_round_bread_1            24   195\n",
      "                      put_donut_in_pot            22   189\n",
      "                put_bread_slice_in_pot            21   186\n",
      "       put_yellow_pepper_in_top_drawer            20   183\n",
      "              put_yellow_pepper_in_pot            23   177\n",
      "         put_toast_in_top_white_drawer            22   174\n",
      "             put_triangle_bread_in_pot            22   171\n",
      "                      put_bread_in_pot            16   147\n",
      "                         pick_up_bread            16   144\n",
      "              green_pepper_in_basket_1            10   114\n",
      "                        put_egg_in_pot            10    78\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š Test ç»Ÿè®¡\n",
      "============================================================\n",
      "  â€¢ åŠ¨ä½œç±»å‹æ•°é‡: 10\n",
      "  â€¢ Trajectoryæ€»æ•°: 490\n",
      "  â€¢ æ ·æœ¬æ€»æ•°: 4977\n",
      "\n",
      "è¯¦ç»†ç»Ÿè®¡ï¼š\n",
      "                   åŠ¨ä½œç±»å‹  Trajectoryæ•°é‡  æ ·æœ¬æ•°é‡\n",
      " close_top_white_drawer            55   612\n",
      "        close_trash_can            55   594\n",
      "          cover_pot_lid            51   525\n",
      "  open_top_white_drawer            46   516\n",
      "   red_pepper_in_basket            47   489\n",
      "    red_pepper_on_table            47   468\n",
      "         open_trash_can            48   456\n",
      "yellow_pepper_in_basket            45   453\n",
      "   put_pot_lid_on_table            48   435\n",
      "           open_pot_lid            48   429\n",
      "============================================================\n",
      "\n",
      "ğŸ“Œ æ•´ä½“å æ¯”ï¼ˆæŒ‰æ ·æœ¬/trajectory/ç±»åˆ«æ•°ï¼‰\n",
      "  â€¢ Test æ ·æœ¬å æ¯”: 4977/23508 = 21.17%\n",
      "  â€¢ Test Trajectoryå æ¯”: 490/2624 = 18.67%\n",
      "  â€¢ Test ç±»åˆ«å æ¯”: 10/74 = 13.51%\n",
      "\n",
      "âœ… åˆ‡åˆ†å®Œæˆã€‚ç±»åˆ«äº’æ–¥ï¼šTest = 10 ç±»ï¼›Train = å…¶ä½™å…¨éƒ¨ç±»åˆ«ã€‚\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# ===================== é…ç½®åŒº =====================\n",
    "INPUT_JSONL = '/home/runsheng/personal_3/qiancx/ProgressLM/data/h5_ur_1rgb_converted.jsonl'  # æ›¿æ¢ä¸ºä½ çš„æ–‡ä»¶è·¯å¾„\n",
    "OUTPUT_DIR  = './home/runsheng/personal_3/qiancx/Sources/datasets/robomind/progrsslm/annotation/h5_ur_1rgb'  # è¾“å‡ºç›®å½•ï¼ˆä¼šè‡ªåŠ¨åˆ›å»ºï¼‰\n",
    "TRAIN_JSONL = os.path.join(OUTPUT_DIR, 'h5_ur_1rgb_train.jsonl')\n",
    "TEST_JSONL  = os.path.join(OUTPUT_DIR, 'h5_ur_1rgb_test.jsonl')\n",
    "SAVE_CLASS_LISTS = False                  # æ˜¯å¦åŒæ—¶å¯¼å‡ºç±»åˆ«æ¸…å•\n",
    "\n",
    "# æµ‹è¯•é›†ç±»åˆ«ï¼ˆ10ç±»ï¼‰\n",
    "TEST_CLASSES = {\n",
    "    'close_top_white_drawer',\n",
    "    'close_trash_can',\n",
    "    'cover_pot_lid',\n",
    "    'open_top_white_drawer',\n",
    "    'red_pepper_in_basket',\n",
    "    'red_pepper_on_table',\n",
    "    'open_trash_can',\n",
    "    'put_pot_lid_on_table',\n",
    "    'open_pot_lid',\n",
    "    'yellow_pepper_in_basket',\n",
    "}\n",
    "# =================================================\n",
    "\n",
    "def ensure_dir(path: str):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def extract_action_type(id_string: str):\n",
    "    \"\"\"\n",
    "    ä»IDä¸­æå–åŠ¨ä½œç±»å‹\n",
    "    çº¦å®š: IDå½¢å¦‚ æ•°æ®é›†å/åŠ¨ä½œç±»å‹/æ—¶é—´æˆ³ï¼ˆè‡³å°‘åŒ…å«ä¸¤æ®µï¼‰\n",
    "    \"\"\"\n",
    "    parts = id_string.split('/')\n",
    "    if len(parts) >= 2:\n",
    "        return parts[1]\n",
    "    return None\n",
    "\n",
    "def load_jsonl(jsonl_path: str):\n",
    "    data = []\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                data.append(json.loads(line))\n",
    "    print(f\"âœ“ æˆåŠŸåŠ è½½ {len(data)} æ¡æ•°æ®\")\n",
    "    return data\n",
    "\n",
    "def split_by_classes(data, test_classes: set):\n",
    "    \"\"\"æŒ‰ç…§ç±»åˆ«é›†åˆåˆ’åˆ† train/testï¼ˆç±»åˆ«äº’æ–¥ï¼‰\"\"\"\n",
    "    train, test = [], []\n",
    "    for item in data:\n",
    "        act = extract_action_type(item.get('id', ''))\n",
    "        if act in test_classes:\n",
    "            test.append(item)\n",
    "        else:\n",
    "            train.append(item)\n",
    "    return train, test\n",
    "\n",
    "def dump_jsonl(data, out_path: str):\n",
    "    with open(out_path, 'w', encoding='utf-8') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "def summarize_split(data, title: str):\n",
    "    \"\"\"\n",
    "    è¾“å‡ºè¯¥ split çš„åŸºç¡€ç»Ÿè®¡ï¼ˆæ ·æœ¬/trajectory/æŒ‰ç±»åˆ«åˆ†å¸ƒï¼‰\n",
    "    æ³¨æ„ï¼šæ²¿ç”¨ä½ åŸè„šæœ¬å£å¾„ï¼Œä»¥å®Œæ•´ id ä½œä¸º trajectory çš„å”¯ä¸€æ ‡è¯†\n",
    "    \"\"\"\n",
    "    action_data = defaultdict(lambda: {'trajectories': set(), 'samples': 0})\n",
    "    for item in data:\n",
    "        id_string = item.get('id', '')\n",
    "        action_type = extract_action_type(id_string)\n",
    "        if action_type:\n",
    "            action_data[action_type]['trajectories'].add(id_string)\n",
    "            action_data[action_type]['samples'] += 1\n",
    "\n",
    "    rows = []\n",
    "    for act, info in action_data.items():\n",
    "        rows.append({\n",
    "            'åŠ¨ä½œç±»å‹': act,\n",
    "            'Trajectoryæ•°é‡': len(info['trajectories']),\n",
    "            'æ ·æœ¬æ•°é‡': info['samples']\n",
    "        })\n",
    "    df = pd.DataFrame(rows).sort_values('æ ·æœ¬æ•°é‡', ascending=False)\n",
    "\n",
    "    n_actions = len(df)\n",
    "    n_traj = int(df['Trajectoryæ•°é‡'].sum()) if not df.empty else 0\n",
    "    n_samples = int(df['æ ·æœ¬æ•°é‡'].sum()) if not df.empty else 0\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"ğŸ“Š {title} ç»Ÿè®¡\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"  â€¢ åŠ¨ä½œç±»å‹æ•°é‡: {n_actions}\")\n",
    "    print(f\"  â€¢ Trajectoryæ€»æ•°: {n_traj}\")\n",
    "    print(f\"  â€¢ æ ·æœ¬æ€»æ•°: {n_samples}\")\n",
    "    if not df.empty:\n",
    "        print(\"\\nè¯¦ç»†ç»Ÿè®¡ï¼š\")\n",
    "        print(df.to_string(index=False))\n",
    "    else:\n",
    "        print(\"\\nè¯¥ split ä¸ºç©ºã€‚\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "    return {'n_actions': n_actions, 'n_traj': n_traj, 'n_samples': n_samples, 'df': df}\n",
    "\n",
    "def main():\n",
    "    ensure_dir(OUTPUT_DIR)\n",
    "\n",
    "    # 1) è¯»å…¥\n",
    "    data = load_jsonl(INPUT_JSONL)\n",
    "\n",
    "    # 2) åˆ‡åˆ†ï¼ˆç±»åˆ«äº’æ–¥ï¼‰\n",
    "    train, test = split_by_classes(data, TEST_CLASSES)\n",
    "\n",
    "    # 3) å†™å‡º\n",
    "    dump_jsonl(train, TRAIN_JSONL)\n",
    "    dump_jsonl(test, TEST_JSONL)\n",
    "    print(f\"âœ“ å·²å†™å‡º Train: {len(train)} æ¡ -> {TRAIN_JSONL}\")\n",
    "    print(f\"âœ“ å·²å†™å‡º Test : {len(test)} æ¡ -> {TEST_JSONL}\")\n",
    "\n",
    "    # 4) ç»Ÿè®¡ï¼ˆåˆ†åˆ«å¯¹ Train/Testï¼‰\n",
    "    train_stat = summarize_split(train, \"Train\")\n",
    "    test_stat  = summarize_split(test,  \"Test\")\n",
    "\n",
    "    # 5) æ±‡æ€»å æ¯”ï¼ˆä¾¿äº sanity checkï¼‰\n",
    "    total_samples = train_stat['n_samples'] + test_stat['n_samples']\n",
    "    total_traj    = train_stat['n_traj'] + test_stat['n_traj']\n",
    "    total_actions = train_stat['n_actions'] + test_stat['n_actions']\n",
    "\n",
    "    print(\"ğŸ“Œ æ•´ä½“å æ¯”ï¼ˆæŒ‰æ ·æœ¬/trajectory/ç±»åˆ«æ•°ï¼‰\")\n",
    "    if total_samples > 0:\n",
    "        print(f\"  â€¢ Test æ ·æœ¬å æ¯”: {test_stat['n_samples']}/{total_samples} = {test_stat['n_samples']/total_samples:.2%}\")\n",
    "    if total_traj > 0:\n",
    "        print(f\"  â€¢ Test Trajectoryå æ¯”: {test_stat['n_traj']}/{total_traj} = {test_stat['n_traj']/total_traj:.2%}\")\n",
    "    if total_actions > 0:\n",
    "        print(f\"  â€¢ Test ç±»åˆ«å æ¯”: {test_stat['n_actions']}/{total_actions} = {test_stat['n_actions']/total_actions:.2%}\")\n",
    "\n",
    "    # 6) å¯é€‰ï¼šå¯¼å‡ºç±»åˆ«æ¸…å•\n",
    "    if SAVE_CLASS_LISTS:\n",
    "        with open(os.path.join(OUTPUT_DIR, 'test_classes.txt'), 'w', encoding='utf-8') as f:\n",
    "            f.write('\\n'.join(sorted(TEST_CLASSES)))\n",
    "        # train ç±»åˆ« = å…¨ä½“å‡ºç°è¿‡çš„ç±»åˆ« - TEST_CLASSES\n",
    "        all_actions = set()\n",
    "        for item in data:\n",
    "            act = extract_action_type(item.get('id', ''))\n",
    "            if act:\n",
    "                all_actions.add(act)\n",
    "        train_actions = sorted(all_actions - TEST_CLASSES)\n",
    "        with open(os.path.join(OUTPUT_DIR, 'train_classes.txt'), 'w', encoding='utf-8') as f:\n",
    "            f.write('\\n'.join(train_actions))\n",
    "        print(\"âœ“ å·²ä¿å­˜ test_classes.txt ä¸ train_classes.txt\")\n",
    "\n",
    "    print(\"\\nâœ… åˆ‡åˆ†å®Œæˆã€‚ç±»åˆ«äº’æ–¥ï¼šTest = 10 ç±»ï¼›Train = å…¶ä½™å…¨éƒ¨ç±»åˆ«ã€‚\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905c4c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# ===================== é…ç½®åŒº =====================\n",
    "INPUT_JSONL = '/projects/b1222/userdata/jianshu/chengxuan/ProgressLM/data/raw/visual_demo/visual_h5_ur_1rgb_train.jsonl'\n",
    "OUTPUT_DIR  = '/projects/b1222/userdata/jianshu/chengxuan/ProgressLM/data/train/visual_demo'\n",
    "SFT_JSONL   = os.path.join(OUTPUT_DIR, 'visual_h5_ur_1rgb_sft.jsonl')\n",
    "RL_JSONL    = os.path.join(OUTPUT_DIR, 'visual_h5_ur_1rgb_rl.jsonl')\n",
    "SPLIT_RATIO = 0.1   # SFT å  20%ï¼ŒRL å  80%\n",
    "SEED = 42\n",
    "# =================================================\n",
    "\n",
    "def ensure_dir(path: str):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def extract_action_type(id_string: str):\n",
    "    parts = id_string.split('/')\n",
    "    if len(parts) >= 2:\n",
    "        return parts[1]\n",
    "    return None\n",
    "\n",
    "def load_jsonl(jsonl_path: str):\n",
    "    data = []\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                data.append(json.loads(line))\n",
    "    print(f\"âœ“ æˆåŠŸåŠ è½½ {len(data)} æ¡æ•°æ®\")\n",
    "    return data\n",
    "\n",
    "def group_by_trajectory(data):\n",
    "    \"\"\"\n",
    "    å°†æ•°æ®æŒ‰å®Œæ•´ idï¼ˆå³è½¨è¿¹ï¼‰èšåˆã€‚\n",
    "    è¿”å›: {action_type: {trajectory_id: [items...] } }\n",
    "    \"\"\"\n",
    "    grouped = defaultdict(lambda: defaultdict(list))\n",
    "    for item in data:\n",
    "        tid = item.get('id', '')\n",
    "        act = extract_action_type(tid)\n",
    "        if act:\n",
    "            grouped[act][tid].append(item)\n",
    "    return grouped\n",
    "\n",
    "def dump_jsonl(data, out_path: str):\n",
    "    with open(out_path, 'w', encoding='utf-8') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "def main():\n",
    "    random.seed(SEED)\n",
    "    ensure_dir(OUTPUT_DIR)\n",
    "\n",
    "    # 1) åŠ è½½æ•°æ®\n",
    "    data = load_jsonl(INPUT_JSONL)\n",
    "\n",
    "    # 2) æŒ‰ç±»åˆ« & è½¨è¿¹åˆ†ç»„\n",
    "    grouped = group_by_trajectory(data)\n",
    "    print(f\"âœ“ å…±å‘ç° {len(grouped)} ä¸ªç±»åˆ«ï¼ˆåŸºäº extract_action_typeï¼‰\\n\")\n",
    "\n",
    "    # 3) åˆ†å±‚åˆ’åˆ† SFT / RL\n",
    "    sft_data, rl_data = [], []\n",
    "\n",
    "    category_stats = []  # ç”¨äºæ‰“å°ç»Ÿè®¡\n",
    "    for act, traj_map in grouped.items():\n",
    "        traj_ids = list(traj_map.keys())\n",
    "        random.shuffle(traj_ids)\n",
    "\n",
    "        # è®¡ç®— SFT è½¨è¿¹æ•°é‡\n",
    "        sft_count = int(len(traj_ids) * SPLIT_RATIO)\n",
    "        sft_traj = traj_ids[:sft_count]\n",
    "        rl_traj = traj_ids[sft_count:]\n",
    "\n",
    "        # æ”¶é›†æ•°æ®\n",
    "        sft_items = [item for tid in sft_traj for item in traj_map[tid]]\n",
    "        rl_items = [item for tid in rl_traj for item in traj_map[tid]]\n",
    "\n",
    "        sft_data.extend(sft_items)\n",
    "        rl_data.extend(rl_items)\n",
    "\n",
    "        # è®°å½•ç»Ÿè®¡\n",
    "        category_stats.append({\n",
    "            'ç±»åˆ«': act,\n",
    "            'SFT_è½¨è¿¹æ•°': len(sft_traj),\n",
    "            'RL_è½¨è¿¹æ•°': len(rl_traj),\n",
    "            'SFT_æ ·æœ¬æ•°': len(sft_items),\n",
    "            'RL_æ ·æœ¬æ•°': len(rl_items),\n",
    "        })\n",
    "\n",
    "    # 4) å†™å‡ºæ–‡ä»¶\n",
    "    dump_jsonl(sft_data, SFT_JSONL)\n",
    "    dump_jsonl(rl_data, RL_JSONL)\n",
    "\n",
    "    # 5) æ‰“å°ç»Ÿè®¡ä¿¡æ¯\n",
    "    print(\"\\n====== æ•°æ®é›†åˆ’åˆ†å®Œæˆ ======\")\n",
    "    print(f\"æ€»ç±»åˆ«æ•°: {len(category_stats)}\")\n",
    "    print(f\"SFT : RL = {SPLIT_RATIO:.0%} : {1-SPLIT_RATIO:.0%}\\n\")\n",
    "\n",
    "    print(\"ç±»åˆ«åˆ†å¸ƒç»Ÿè®¡:\")\n",
    "    print(\"------------------------------------------------------------\")\n",
    "    print(f\"{'ç±»åˆ«':25s} | {'SFTè½¨è¿¹':7s} {'RLè½¨è¿¹':7s} {'SFTæ ·æœ¬':7s} {'RLæ ·æœ¬':7s}\")\n",
    "    print(\"------------------------------------------------------------\")\n",
    "    for stat in category_stats:\n",
    "        print(f\"{stat['ç±»åˆ«']:25s} | \"\n",
    "              f\"{stat['SFT_è½¨è¿¹æ•°']:7d} {stat['RL_è½¨è¿¹æ•°']:7d} \"\n",
    "              f\"{stat['SFT_æ ·æœ¬æ•°']:7d} {stat['RL_æ ·æœ¬æ•°']:7d}\")\n",
    "    print(\"------------------------------------------------------------\")\n",
    "    print(f\"SFT æ€»æ ·æœ¬: {len(sft_data)}   RL æ€»æ ·æœ¬: {len(rl_data)}\")\n",
    "    print(f\"ğŸ‘‰ è¾“å‡ºæ–‡ä»¶å·²ç”Ÿæˆ:\\n   - {SFT_JSONL}\\n   - {RL_JSONL}\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen3vl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
