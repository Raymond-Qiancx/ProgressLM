<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="ProgressLM: Towards Progress Reasoning in Vision-Language Models">
    <meta name="keywords" content="Progress Reasoning, Vision-Language Models, VLM, Machine Learning, Deep Learning">
    <meta name="author" content="Jianshu Zhang, Chengxuan Qian, Haosen Sun, Haoran Lu, Dingcheng Wang, Letian Xue, Han Liu">

    <!-- Open Graph -->
    <meta property="og:title" content="ProgressLM">
    <meta property="og:description" content="Towards Progress Reasoning in Vision-Language Models">
    <meta property="og:type" content="website">

    <title>ProgressLM - Progress Reasoning in Vision-Language Models</title>

    <!-- Stylesheets -->
    <link rel="stylesheet" href="css/style.css">

    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
</head>
<body>
    <!-- Hero Section -->
    <section id="hero">
        <div class="container">
            <div class="title-with-icon">
                <img src="imgs/icon.png" alt="ProgressLM Icon" class="title-icon">
                <h1>ProgressLM</h1>
            </div>
            <p class="subtitle">Towards Progress Reasoning in Vision-Language Models</p>

            <div class="buttons">
                <a href="#" class="btn btn-primary">
                    <i class="fas fa-file-pdf"></i> Paper
                </a>
                <a href="https://github.com/ProgressLM/ProgressLM" class="btn" target="_blank">
                    <i class="fab fa-github"></i> Code
                </a>
                <a href="https://huggingface.co/Raymond-Qiancx/ProgressLM-3B-SFT" class="btn" target="_blank">
                    <i class="fas fa-cube"></i> SFT Model
                </a>
                <a href="https://huggingface.co/Raymond-Qiancx/ProgressLM-3B-RL" class="btn" target="_blank">
                    <i class="fas fa-cube"></i> RL Model
                </a>
                <a href="https://huggingface.co/datasets/Raymond-Qiancx/ProgressLM-Dataset" class="btn" target="_blank">
                    <i class="fas fa-database"></i> Dataset
                </a>
            </div>

            <div class="authors-box">
                <div class="authors">
                    <span class="author">Jianshu Zhang<sup>*</sup></span>
                    <span class="author">Chengxuan Qian<sup>*</sup></span>
                    <span class="author">Haosen Sun</span>
                    <span class="author">Haoran Lu</span>
                    <span class="author">Dingcheng Wang</span>
                    <span class="author">Letian Xue</span>
                    <span class="author">Han Liu</span>
                </div>
                <p class="equal-contribution"><sup>*</sup> Equal Contribution</p>
            </div>

            <div class="teaser-image">
                <img src="imgs/teaser.png" alt="ProgressLM Teaser" style="width: 100%; max-width: 900px; margin-top: 32px; border-radius: 8px;">
                <p class="teaser-caption">
                    Given a task demonstration and a single observation, the goal is to estimate how much of the task has already been completed.
                    Direct prediction can often judge whether the task is unfinished, but struggles to assign a well-calibrated progress score.
                    Progress reasoning instead follows a coarse-to-fine process: it first performs <em>episodic retrieval</em> to coarsely locate the observation along the demonstrated task,
                    then applies <em>mental simulation</em> to imagine the transition from the retrieved anchor to the current observation, enabling a fine-grained estimate of completed progress.
                </p>
            </div>
        </div>
    </section>

    <!-- Abstract Section -->
    <section id="abstract">
        <div class="container">
            <h2>Abstract</h2>
            <p>
                Estimating task progress requires reasoning over long-horizon dynamics rather than recognizing static visual content.
                While modern Vision-Language Models (VLMs) excel at describing what is visible, it remains unclear whether they can infer how far a task has progressed from partial observations.
                In this work, we introduce <strong>Progress-Bench</strong>, a benchmark for systematically evaluating progress reasoning in VLMs.
                We further explore a human-inspired two-stage progress reasoning paradigm that combines <strong>episodic retrieval</strong> with <strong>mental simulation</strong>.
            </p>
            <p>
                We instantiate this paradigm through both training-free prompting and a training-based approach built on an automatically curated dataset, <strong>ProgressLM-45K</strong>.
                Evaluating 14 VLMs on Progress-Bench, we find that current models struggle to reliably estimate task progress.
                While training-free prompting that enforces structured progress reasoning yields improvements, these gains are limited and model-dependent.
                In contrast, <strong>ProgressLM-3B</strong> achieves consistent improvements in accuracy, robustness to viewpoint variation, and calibrated handling of unanswerable cases, even at small model scale.
                Further analyses reveal characteristic error patterns of existing VLMs and clarify when and why progress reasoning succeeds or fails.
            </p>
        </div>
    </section>

    <!-- Benchmark Design Section -->
    <section id="benchmark-design">
        <div class="container">
            <h2>Benchmark Design</h2>
            <p style="margin-bottom: 24px; color: var(--text-secondary); text-align: center;">
                We design our benchmark along three key dimensions to comprehensively evaluate progress reasoning capabilities.
            </p>
            <div class="benchmark-design-image">
                <img src="imgs/bench_curation.png" alt="Benchmark Design" style="width: 100%; max-width: 1000px; margin: 0 auto; display: block; border-radius: 8px;">
            </div>
            <div class="dimension-cards" style="display: grid; grid-template-columns: repeat(auto-fit, minmax(280px, 1fr)); gap: 20px; margin-top: 32px;">
                <div class="dimension-card" style="background: var(--card-bg); padding: 20px; border-radius: 8px; border: 1px solid var(--border-color);">
                    <h3 style="color: var(--accent-color); margin-bottom: 8px;"><i class="fas fa-eye"></i> Demonstration Modality</h3>
                    <p style="color: var(--text-secondary);">Text-based vs. vision-based demonstrations to test different input modalities.</p>
                </div>
                <div class="dimension-card" style="background: var(--card-bg); padding: 20px; border-radius: 8px; border: 1px solid var(--border-color);">
                    <h3 style="color: var(--accent-color); margin-bottom: 8px;"><i class="fas fa-camera"></i> Viewpoint Correspondence</h3>
                    <p style="color: var(--text-secondary);">Same-view vs. cross-view settings to evaluate spatial reasoning robustness.</p>
                </div>
                <div class="dimension-card" style="background: var(--card-bg); padding: 20px; border-radius: 8px; border: 1px solid var(--border-color);">
                    <h3 style="color: var(--accent-color); margin-bottom: 8px;"><i class="fas fa-question-circle"></i> Answerability</h3>
                    <p style="color: var(--text-secondary);">Well-defined vs. ambiguous cases to test the model's ability to recognize unanswerable queries.</p>
                </div>
            </div>
        </div>
    </section>

    <!-- Method Section -->
    <section id="method">
        <div class="container">
            <h2>Method</h2>
            <div class="placeholder" style="margin-bottom: 32px;">
                <i class="fas fa-diagram-project"></i>
                <p>Method overview figure coming soon...</p>
            </div>

            <div class="method-overview">
                <div class="method-card">
                    <h3><i class="fas fa-graduation-cap"></i> SFT Training</h3>
                    <p>
                        Supervised fine-tuning using LLaMA-Factory framework.
                        Supports LoRA, QLoRA, and full fine-tuning with Qwen2.5-VL and Qwen3-VL architectures.
                    </p>
                </div>
                <div class="method-card">
                    <h3><i class="fas fa-brain"></i> RL Training (GRPO)</h3>
                    <p>
                        Reinforcement learning with Group Relative Policy Optimization using EasyR1 framework.
                        Provides distributed training support with FSDP.
                    </p>
                </div>
                <div class="method-card">
                    <h3><i class="fas fa-chart-line"></i> Evaluation</h3>
                    <p>
                        Comprehensive evaluation pipeline with multiple metrics including VOC, Score Error,
                        Ref Error, and N/A Recall for progress reasoning assessment.
                    </p>
                </div>
            </div>
        </div>
    </section>

    <!-- Benchmarks Section -->
    <section id="benchmarks">
        <div class="container">
            <h2>Benchmarks</h2>

            <div class="statistics-image" style="margin-bottom: 32px;">
                <img src="imgs/statistics.png" alt="Dataset Statistics" style="width: 100%; max-width: 900px; margin: 0 auto; display: block; border-radius: 8px;">
            </div>

            <div class="benchmark-grid">
                <div class="benchmark-category">
                    <h3><i class="fas fa-code"></i> prog-bench</h3>
                    <p style="margin-bottom: 16px; color: var(--text-secondary);">
                        Programmatic task benchmarks for progress reasoning evaluation
                    </p>
                    <ul>
                        <li><i class="fas fa-check"></i> Text Demo (Normal)</li>
                        <li><i class="fas fa-check"></i> Text Demo (Unanswerable)</li>
                        <li><i class="fas fa-check"></i> Visual Demo (Same View)</li>
                        <li><i class="fas fa-check"></i> Visual Demo (Cross View)</li>
                        <li><i class="fas fa-check"></i> Visual Demo (Unanswerable)</li>
                    </ul>
                </div>

                <div class="benchmark-category">
                    <h3><i class="fas fa-person-walking"></i> human-bench</h3>
                    <p style="margin-bottom: 16px; color: var(--text-secondary);">
                        Human activity benchmarks for progress reasoning evaluation
                    </p>
                    <ul>
                        <li><i class="fas fa-check"></i> Text Demo (Human Activities)</li>
                        <li><i class="fas fa-check"></i> Visual Demo (Human Activities)</li>
                    </ul>
                </div>
            </div>

            <div class="metrics">
                <h3><i class="fas fa-ruler"></i> Evaluation Metrics</h3>
                <div class="metrics-grid">
                    <div class="metric-item">
                        <strong>VOC</strong>
                        <span>Trajectory Order Consistency</span>
                    </div>
                    <div class="metric-item">
                        <strong>Score Error</strong>
                        <span>Normalized Progress Score Error</span>
                    </div>
                    <div class="metric-item">
                        <strong>Ref Error</strong>
                        <span>Reference Step Index Error</span>
                    </div>
                    <div class="metric-item">
                        <strong>N/A Recall</strong>
                        <span>Unanswerable Detection Recall</span>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Results Section -->
    <section id="results">
        <div class="container">
            <h2>Supported Models</h2>
            <p style="margin-bottom: 24px; color: var(--text-secondary); text-align: center;">
                We provide evaluation scripts for a wide range of vision-language models.
            </p>
            <div class="models-list">
                <div class="model-badge">
                    <i class="fas fa-robot"></i> Qwen2.5-VL (3B, 7B, 32B, 72B)
                </div>
                <div class="model-badge">
                    <i class="fas fa-robot"></i> Qwen3-VL (2B, 4B, 8B, 32B, 30B-MoE)
                </div>
                <div class="model-badge">
                    <i class="fas fa-robot"></i> InternVL
                </div>
                <div class="model-badge">
                    <i class="fas fa-robot"></i> GPT-4V / GPT-4o
                </div>
            </div>
            <p style="margin-top: 16px; color: var(--text-secondary); text-align: center; font-size: 0.9em;">
                Qwen3-VL supports both <strong>thinking</strong> and <strong>non-thinking</strong> modes for evaluation.
            </p>
        </div>
    </section>

    <!-- Citation Section -->
    <section id="citation">
        <div class="container">
            <h2>Citation</h2>
            <p>If you find our work useful, please consider citing:</p>
            <div class="bibtex-container">
                <pre id="bibtex-content">@article{zhang2025progresslm,
  title={ProgressLM: Towards Progress Reasoning in Vision-Language Models},
  author={Zhang, Jianshu and Qian, Chengxuan and Sun, Haosen and Lu, Haoran and Wang, Dingcheng and Xue, Letian and Liu, Han},
  journal={arXiv preprint arXiv:2505.XXXXX},
  year={2025}
}</pre>
                <button id="copy-btn"><i class="fas fa-copy"></i> Copy</button>
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer>
        <div class="container">
            <p>MIT License - Copyright 2025 ProgressLM Team</p>
            <p>
                <a href="https://github.com/ProgressLM/ProgressLM" target="_blank">
                    <i class="fab fa-github"></i> GitHub Repository
                </a>
            </p>
        </div>
    </footer>

    <!-- Scripts -->
    <script src="js/main.js"></script>
</body>
</html>
