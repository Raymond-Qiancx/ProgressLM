<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="ProgressLM: Towards Progress Reasoning in Vision-Language Models">
    <meta name="keywords" content="Progress Reasoning, Vision-Language Models, VLM, Machine Learning, Deep Learning">
    <meta name="author" content="Jianshu Zhang, Chengxuan Qian, Haosen Sun, Haoran Lu, Dingcheng Wang, Letian Xue, Han Liu">

    <!-- Open Graph -->
    <meta property="og:title" content="ProgressLM">
    <meta property="og:description" content="Towards Progress Reasoning in Vision-Language Models">
    <meta property="og:type" content="website">

    <title>ProgressLM - Progress Reasoning in Vision-Language Models</title>

    <!-- Favicon -->
    <link rel="icon" type="image/png" href="imgs/icon.png">
    <link rel="apple-touch-icon" href="imgs/icon.png">

    <!-- Stylesheets -->
    <link rel="stylesheet" href="css/style.css">

    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
</head>
<body>
    <!-- Hero Section -->
    <section id="hero">
        <div class="container">
            <div class="title-with-icon">
                <img src="imgs/icon.png" alt="ProgressLM Icon" class="title-icon">
                <h1>ProgressLM</h1>
            </div>
            <p class="subtitle">Towards Progress Reasoning in Vision-Language Models</p>

            <div class="buttons">
                <a href="#" class="btn btn-primary">
                    <i class="fas fa-file-pdf"></i> Paper
                </a>
                <a href="https://github.com/ProgressLM/ProgressLM" class="btn" target="_blank">
                    <i class="fab fa-github"></i> Code
                </a>
                <a href="https://huggingface.co/Raymond-Qiancx/ProgressLM-3B-SFT" class="btn" target="_blank">
                    <i class="fas fa-cube"></i> SFT Model
                </a>
                <a href="https://huggingface.co/Raymond-Qiancx/ProgressLM-3B-RL" class="btn" target="_blank">
                    <i class="fas fa-cube"></i> RL Model
                </a>
                <a href="https://huggingface.co/datasets/Raymond-Qiancx/ProgressLM-Dataset" class="btn" target="_blank">
                    <i class="fas fa-database"></i> Dataset
                </a>
            </div>

            <div class="authors-box">
                <div class="authors">
                    <span class="author"><a href="https://sterzhang.github.io/" target="_blank">Jianshu Zhang</a><sup>*</sup></span>
                    <span class="author"><a href="https://qiancx.com/" target="_blank">Chengxuan Qian</a><sup>*</sup></span>
                    <span class="author">Haosen Sun</span>
                    <span class="author">Haoran Lu</span>
                    <span class="author">Dingcheng Wang</span>
                    <span class="author">Letian Xue</span>
                    <span class="author"><a href="https://magics.cs.northwestern.edu/people.html" target="_blank">Han Liu</a></span>
                </div>
                <p class="equal-contribution"><sup>*</sup> Equal Contribution</p>
            </div>

            <div class="teaser-image">
                <img src="imgs/teaser.png" alt="ProgressLM Teaser" style="width: 100%; max-width: 900px; margin-top: 32px; border-radius: 8px;">
                <p class="teaser-caption">
                    <strong><em>Can vision-language models acquire progress estimation as a general reasoning capability from a single observation?</em></strong> Given a task demonstration and a single observation, the goal is to estimate how much of the task has already been completed.
                    Direct prediction can often judge whether the task is unfinished, but struggles to assign a well-calibrated progress score.
                    Progress reasoning instead follows a coarse-to-fine process: it first performs <em>episodic retrieval</em> to coarsely locate the observation along the demonstrated task,
                    then applies <em>mental simulation</em> to imagine the transition from the retrieved anchor to the current observation, enabling a fine-grained estimate of completed progress.
                </p>
            </div>
        </div>
    </section>

    <!-- Abstract Section -->
    <section id="abstract">
        <div class="container">
            <h2>Abstract</h2>
            <p>
                Estimating task progress requires reasoning over long-horizon dynamics rather than recognizing static visual content. While modern Vision-Language Models (VLMs) excel at describing what is visible, it remains unclear whether they can infer how far a task has progressed from partial observations. In this work, we introduce <strong>Progress-Bench</strong>, a benchmark for systematically evaluating progress reasoning in VLMs. We further explore a human-inspired two-stage progress reasoning paradigm that combines <strong>episodic retrieval</strong> with <strong>mental simulation</strong>. We instantiate this paradigm through both training-free prompting and a training-based approach built on an automatically curated dataset, <strong>ProgressLM-45K</strong>. Evaluating 14 VLMs on Progress-Bench, we find that current models struggle to reliably estimate task progress. While training-free prompting that enforces structured progress reasoning yields improvements, these gains are limited and model-dependent. In contrast, <strong>ProgressLM-3B</strong> achieves consistent improvements in accuracy, robustness to viewpoint variation, and calibrated handling of unanswerable cases, even at small model scale. Further analyses reveal characteristic error patterns of existing VLMs and clarify when and why progress reasoning succeeds or fails.
            </p>
        </div>
    </section>

    <!-- Progress Annotation Section -->
    <section id="progress-annotation">
        <div class="container">
            <details class="collapsible-section" open>
                <summary class="collapsible-title">
                    <span>How do we annotate progress under controlled shifts?</span>
                    <span class="collapse-icon"></span>
                </summary>
                <div class="collapsible-content">
                    <div class="annotation-image" style="margin-bottom: 24px;">
                        <img src="imgs/bench_curation.png" alt="Progress Annotation" style="width: 100%; border-radius: 8px;">
                    </div>
                    <p style="text-align: justify; line-height: 1.8;">
                        We construct progress data by pairing each task demonstration with a single observation sampled from intermediate or boundary moments during execution, and assign its progress label via temporal interpolation between adjacent key steps. To systematically probe progress reasoning beyond static matching, we introduce controlled shifts along three dimensions: <strong>demonstration modality</strong> (vision-based vs. text-based), <strong>viewpoint correspondence</strong> (same-view vs. cross-view for vision demos), and <strong>answerability</strong>, where semantic mismatches are injected so that progress becomes ill-defined and the correct output is N/A.
                    </p>
                </div>
            </details>
        </div>
    </section>

    <!-- Data Scaling Section -->
    <section id="data-scaling">
        <div class="container">
            <details class="collapsible-section" open>
                <summary class="collapsible-title">
                    <span>Scaling progress data across robots and objects</span>
                    <span class="collapse-icon"></span>
                </summary>
                <div class="collapsible-content">
                    <div class="scaling-images" style="margin-bottom: 24px;">
                        <img src="imgs/statistics.png" alt="Statistics" style="width: 100%; border-radius: 8px; margin-bottom: 16px;">
                        <img src="imgs/bar-stat-combined.png" alt="Bar Statistics" style="width: 100%; border-radius: 8px;">
                    </div>
                    <p style="text-align: justify; line-height: 1.8;">
                        To scale progress reasoning beyond a single embodiment or environment, we curate progress estimation data spanning single-arm manipulators, dual-arm systems, and humanoid robots, together with diverse object interactions. <strong>Progress-Bench</strong> evaluates models on thousands of observation queries grounded in real manipulation trajectories, while <strong>ProgressLM-45K</strong> further expands training coverage with large-scale supervised and reinforcement learning samples. This diversity encourages models to learn transferable progress cuesâ€”tracking long-horizon state evolution rather than overfitting to specific robots, viewpoints, or object appearances.
                    </p>
                </div>
            </details>
        </div>
    </section>

    <!-- Method Section -->
    <section id="method">
        <div class="container">
            <h2>Method</h2>
            <div class="placeholder" style="margin-bottom: 32px;">
                <i class="fas fa-diagram-project"></i>
                <p>Method overview figure coming soon...</p>
            </div>

            <div class="method-overview">
                <div class="method-card">
                    <h3><i class="fas fa-graduation-cap"></i> SFT Training</h3>
                    <p>
                        Supervised fine-tuning using LLaMA-Factory framework.
                        Supports LoRA, QLoRA, and full fine-tuning with Qwen2.5-VL and Qwen3-VL architectures.
                    </p>
                </div>
                <div class="method-card">
                    <h3><i class="fas fa-brain"></i> RL Training (GRPO)</h3>
                    <p>
                        Reinforcement learning with Group Relative Policy Optimization using EasyR1 framework.
                        Provides distributed training support with FSDP.
                    </p>
                </div>
                <div class="method-card">
                    <h3><i class="fas fa-chart-line"></i> Evaluation</h3>
                    <p>
                        Comprehensive evaluation pipeline with multiple metrics including VOC, Score Error,
                        Ref Error, and N/A Recall for progress reasoning assessment.
                    </p>
                </div>
            </div>
        </div>
    </section>

    <!-- Benchmarks Section -->
    <section id="benchmarks">
        <div class="container">
            <h2>Benchmarks</h2>

            <div class="statistics-image" style="margin-bottom: 32px;">
                <img src="imgs/statistics.png" alt="Dataset Statistics" style="width: 100%; max-width: 900px; margin: 0 auto; display: block; border-radius: 8px;">
            </div>

            <div class="benchmark-grid">
                <div class="benchmark-category">
                    <h3><i class="fas fa-code"></i> prog-bench</h3>
                    <p style="margin-bottom: 16px; color: var(--text-secondary);">
                        Programmatic task benchmarks for progress reasoning evaluation
                    </p>
                    <ul>
                        <li><i class="fas fa-check"></i> Text Demo (Normal)</li>
                        <li><i class="fas fa-check"></i> Text Demo (Unanswerable)</li>
                        <li><i class="fas fa-check"></i> Visual Demo (Same View)</li>
                        <li><i class="fas fa-check"></i> Visual Demo (Cross View)</li>
                        <li><i class="fas fa-check"></i> Visual Demo (Unanswerable)</li>
                    </ul>
                </div>

                <div class="benchmark-category">
                    <h3><i class="fas fa-person-walking"></i> human-bench</h3>
                    <p style="margin-bottom: 16px; color: var(--text-secondary);">
                        Human activity benchmarks for progress reasoning evaluation
                    </p>
                    <ul>
                        <li><i class="fas fa-check"></i> Text Demo (Human Activities)</li>
                        <li><i class="fas fa-check"></i> Visual Demo (Human Activities)</li>
                    </ul>
                </div>
            </div>

            <div class="metrics">
                <h3><i class="fas fa-ruler"></i> Evaluation Metrics</h3>
                <div class="metrics-grid">
                    <div class="metric-item">
                        <strong>VOC</strong>
                        <span>Trajectory Order Consistency</span>
                    </div>
                    <div class="metric-item">
                        <strong>Score Error</strong>
                        <span>Normalized Progress Score Error</span>
                    </div>
                    <div class="metric-item">
                        <strong>Ref Error</strong>
                        <span>Reference Step Index Error</span>
                    </div>
                    <div class="metric-item">
                        <strong>N/A Recall</strong>
                        <span>Unanswerable Detection Recall</span>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Results Section -->
    <section id="results">
        <div class="container">
            <h2>Supported Models</h2>
            <p style="margin-bottom: 24px; color: var(--text-secondary); text-align: center;">
                We provide evaluation scripts for a wide range of vision-language models.
            </p>
            <div class="models-list">
                <div class="model-badge">
                    <i class="fas fa-robot"></i> Qwen2.5-VL (3B, 7B, 32B, 72B)
                </div>
                <div class="model-badge">
                    <i class="fas fa-robot"></i> Qwen3-VL (2B, 4B, 8B, 32B, 30B-MoE)
                </div>
                <div class="model-badge">
                    <i class="fas fa-robot"></i> InternVL
                </div>
                <div class="model-badge">
                    <i class="fas fa-robot"></i> GPT-4V / GPT-4o
                </div>
            </div>
            <p style="margin-top: 16px; color: var(--text-secondary); text-align: center; font-size: 0.9em;">
                Qwen3-VL supports both <strong>thinking</strong> and <strong>non-thinking</strong> modes for evaluation.
            </p>
        </div>
    </section>

    <!-- Citation Section -->
    <section id="citation">
        <div class="container">
            <h2>Citation</h2>
            <p>If you find our work useful, please consider citing:</p>
            <div class="bibtex-container">
                <pre id="bibtex-content">@article{zhang2025progresslm,
  title={ProgressLM: Towards Progress Reasoning in Vision-Language Models},
  author={Zhang, Jianshu and Qian, Chengxuan and Sun, Haosen and Lu, Haoran and Wang, Dingcheng and Xue, Letian and Liu, Han},
  journal={arXiv preprint arXiv:2505.XXXXX},
  year={2025}
}</pre>
                <button id="copy-btn"><i class="fas fa-copy"></i> Copy</button>
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer>
        <div class="container">
            <p>MIT License - Copyright 2025 ProgressLM Team</p>
            <p>
                <a href="https://github.com/ProgressLM/ProgressLM" target="_blank">
                    <i class="fab fa-github"></i> GitHub Repository
                </a>
            </p>
        </div>
    </footer>

    <!-- Scripts -->
    <script src="js/main.js"></script>
</body>
</html>
